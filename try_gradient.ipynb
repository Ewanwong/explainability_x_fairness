{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f3b2b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement attention, attention rollout, attention flow, gradient, input x gradient, integrated gradients, deeplift, kernel shap explanations in a differientiable way\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)  # turn off the mem-efficient kernel\n",
    "torch.backends.cuda.enable_flash_sdp(False)  \n",
    "\n",
    "def get_embeddings(model, input_ids, attention_mask=None, token_type_ids=None, position_ids=None):\n",
    "    if hasattr(model, \"distilbert\"):\n",
    "        embeddings = model.distilbert.embeddings(input_ids=input_ids)\n",
    "    elif hasattr(model, \"roberta\"):\n",
    "        embeddings = model.roberta.embeddings(input_ids=input_ids, position_ids=None, token_type_ids=token_type_ids)\n",
    "    elif hasattr(model, \"bert\"):\n",
    "        embeddings = model.bert.embeddings(input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids)\n",
    "    else:\n",
    "        raise ValueError(\"Model not supported\")\n",
    "    embeddings.requires_grad_(True)\n",
    "    return embeddings\n",
    "\n",
    "def model_forward(model, embeddings, attention_mask=None):\n",
    "\n",
    "    head_mask = model.get_head_mask(None, model.config.num_hidden_layers)\n",
    "    #head_mask = [None] * self.model.config.num_hidden_layers\n",
    "\n",
    "    if hasattr(model, \"distilbert\"):\n",
    "        encoder_outputs = model.distilbert.transformer(\n",
    "            embeddings,\n",
    "            attn_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "        )\n",
    "        hidden_state = encoder_outputs[0]\n",
    "        pooled_output = hidden_state[:, 0]\n",
    "        pooled_output = model.pre_classifier(pooled_output)\n",
    "        pooled_output = model.dropout(pooled_output) \n",
    "        logits = model.classifier(pooled_output)\n",
    "\n",
    "    elif hasattr(model, \"roberta\"):\n",
    "        extended_attention_mask = model.get_extended_attention_mask(\n",
    "            attention_mask, embeddings.shape[:2],\n",
    "        )\n",
    "\n",
    "        encoder_outputs = model.roberta.encoder(\n",
    "            embeddings,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        #sequence_output = self.model.roberta.pooler(sequence_output) if self.model.roberta.pooler is not None else None\n",
    "        logits = model.classifier(sequence_output)\n",
    "\n",
    "    elif hasattr(model, \"bert\"):\n",
    "        extended_attention_mask = model.get_extended_attention_mask(\n",
    "            attention_mask, embeddings.shape[:2], embeddings.device\n",
    "        )\n",
    "\n",
    "        encoder_outputs = model.bert.encoder(\n",
    "            embeddings,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = model.bert.pooler(sequence_output) if model.bert.pooler is not None else None\n",
    "        pooled_output = model.dropout(pooled_output)\n",
    "        logits = model.classifier(pooled_output)\n",
    "    else:\n",
    "        raise ValueError(\"Model not supported\")\n",
    "\n",
    "    return logits\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45e3bb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/yifwang/anaconda3/envs/bcos/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from explainer.Explainer_Encoder import *\n",
    "\n",
    "model_name = \"/scratch/yifwang/new_fairness_x_explainability/new_debiased_models_civil/bert_civil_race/no_debiasing\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, output_attentions=True).to(device)\n",
    "model.eval()\n",
    "\n",
    "text1 = \"racist comment? of course not! blacks are incapable of being racists! maxine, time for you to swim back to africa! now that ' s not racist... it is simply honest.\"\n",
    "text2 = \"test test test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bfc4ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/yifwang/anaconda3/envs/bcos/lib/python3.9/site-packages/transformers/modeling_utils.py:1141: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Saliency_L2': [[{'index': 0,\n",
       "    'text': \"racist comment? of course not! blacks are incapable of being racists! maxine, time for you to swim back to africa! now that ' s not racist... it is simply honest.\",\n",
       "    'true_label': 1,\n",
       "    'predicted_class': 1,\n",
       "    'predicted_class_confidence': 0.64649498462677,\n",
       "    'target_class': 1,\n",
       "    'target_class_confidence': 0.64649498462677,\n",
       "    'method': 'Saliency_L2',\n",
       "    'attribution': [('[CLS]', 0.052127160131931305),\n",
       "     ('racist', 0.036963626742362976),\n",
       "     ('comment', 0.05362391099333763),\n",
       "     ('?', 0.021845147013664246),\n",
       "     ('of', 0.008625098504126072),\n",
       "     ('course', 0.014305680058896542),\n",
       "     ('not', 0.01100395992398262),\n",
       "     ('!', 0.020923681557178497),\n",
       "     ('blacks', 0.05848630517721176),\n",
       "     ('are', 0.023970765992999077),\n",
       "     ('incapable', 0.10321283340454102),\n",
       "     ('of', 0.017269112169742584),\n",
       "     ('being', 0.01954365149140358),\n",
       "     ('racist', 0.04027537629008293),\n",
       "     ('##s', 0.01688440516591072),\n",
       "     ('!', 0.022775186225771904),\n",
       "     ('maxi', 0.03658979386091232),\n",
       "     ('##ne', 0.015268146060407162),\n",
       "     (',', 0.011164960451424122),\n",
       "     ('time', 0.013865037821233273),\n",
       "     ('for', 0.008659730665385723),\n",
       "     ('you', 0.011015456169843674),\n",
       "     ('to', 0.008930567651987076),\n",
       "     ('swim', 0.03517177700996399),\n",
       "     ('back', 0.01045815646648407),\n",
       "     ('to', 0.0067703332751989365),\n",
       "     ('africa', 0.016616811975836754),\n",
       "     ('!', 0.014988572336733341),\n",
       "     ('now', 0.01012085098773241),\n",
       "     ('that', 0.010222114622592926),\n",
       "     (\"'\", 0.004411038011312485),\n",
       "     ('s', 0.00776052987203002),\n",
       "     ('not', 0.008931058458983898),\n",
       "     ('racist', 0.02497628703713417),\n",
       "     ('.', 0.010027928277850151),\n",
       "     ('.', 0.008635501377284527),\n",
       "     ('.', 0.007615263573825359),\n",
       "     ('it', 0.010488168336451054),\n",
       "     ('is', 0.012297305278480053),\n",
       "     ('simply', 0.02502736821770668),\n",
       "     ('honest', 0.05094938352704048),\n",
       "     ('.', 0.013699513860046864),\n",
       "     ('[SEP]', 0.02177232876420021)]}]],\n",
       " 'Saliency_mean': [[{'index': 0,\n",
       "    'text': \"racist comment? of course not! blacks are incapable of being racists! maxine, time for you to swim back to africa! now that ' s not racist... it is simply honest.\",\n",
       "    'true_label': 1,\n",
       "    'predicted_class': 1,\n",
       "    'predicted_class_confidence': 0.64649498462677,\n",
       "    'target_class': 1,\n",
       "    'target_class_confidence': 0.64649498462677,\n",
       "    'method': 'Saliency_mean',\n",
       "    'attribution': [('[CLS]', 3.3799296943470836e-06),\n",
       "     ('racist', 9.942547876562458e-06),\n",
       "     ('comment', 3.4027448236884084e-06),\n",
       "     ('?', 1.0982223102473654e-05),\n",
       "     ('of', -5.093836080050096e-06),\n",
       "     ('course', -9.320152457803488e-06),\n",
       "     ('not', 2.2359351703471475e-07),\n",
       "     ('!', -3.537829252309166e-05),\n",
       "     ('blacks', 6.035154910932761e-06),\n",
       "     ('are', -2.6840693863050546e-06),\n",
       "     ('incapable', -1.899918970593717e-05),\n",
       "     ('of', -1.3049721019342542e-06),\n",
       "     ('being', 1.2286053788557183e-05),\n",
       "     ('racist', 1.0033480066340417e-05),\n",
       "     ('##s', -1.1463700502645224e-05),\n",
       "     ('!', -2.2469364921562374e-05),\n",
       "     ('maxi', 3.107791997081222e-07),\n",
       "     ('##ne', -1.953340688487515e-05),\n",
       "     (',', 7.190881660790183e-06),\n",
       "     ('time', -6.267137905524578e-06),\n",
       "     ('for', -6.77369598633959e-06),\n",
       "     ('you', -3.5401385503064375e-06),\n",
       "     ('to', -5.645833425660385e-06),\n",
       "     ('swim', -2.774247604975244e-06),\n",
       "     ('back', -8.732253036214388e-07),\n",
       "     ('to', 2.2093236111686565e-06),\n",
       "     ('africa', 2.4727282834646758e-06),\n",
       "     ('!', -1.553563015477266e-05),\n",
       "     ('now', 8.506276572006755e-06),\n",
       "     ('that', 4.072060619364493e-06),\n",
       "     (\"'\", 2.537173259042902e-06),\n",
       "     ('s', 3.4756249078782275e-06),\n",
       "     ('not', 8.181970770237967e-06),\n",
       "     ('racist', 1.1523185094119981e-05),\n",
       "     ('.', 1.2074847290932667e-05),\n",
       "     ('.', 7.536676093877759e-06),\n",
       "     ('.', 5.021562628826359e-06),\n",
       "     ('it', 6.919437055330491e-06),\n",
       "     ('is', -1.3615160696645034e-06),\n",
       "     ('simply', 1.939827052410692e-05),\n",
       "     ('honest', 1.996597711695358e-05),\n",
       "     ('.', 1.1808515409938991e-05),\n",
       "     ('[SEP]', -5.610017979051918e-06)]}]]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explainer = GradientNPropabationExplainer(model, tokenizer, method=\"Saliency\", baseline=\"pad\")\n",
    "expl = explainer.explain(\n",
    "    texts = [text1],\n",
    "    example_indices=[0],\n",
    "    labels=[1],\n",
    "    only_predicted_classes=True,\n",
    ")\n",
    "expl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c498eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02, 0.0196, 0.0238, 0.017, 0.015, 0.0208, 0.0159, 0.0209, 0.0436, 0.0214, 0.0287, 0.0171, 0.0302, 0.0676, 0.0334, 0.0272, 0.0219, 0.0144, 0.0107, 0.0219, 0.0117, 0.0132, 0.0101, 0.0194, 0.0122, 0.0092, 0.0169, 0.0118, 0.0123, 0.0096, 0.0082, 0.0116, 0.0126, 0.0181, 0.0119, 0.0129, 0.0081, 0.0099, 0.0143, 0.0227, 0.032, 0.0139, 0.0957]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8894000000000001"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attrs = [round(a[1], 4) for a in expl[\"IntegratedGradients_L2\"][0][0][\"attribution\"]]\n",
    "print(attrs)\n",
    "sum(attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74a0cb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_attention_attr(model, input_ids, attention_mask, token_type_ids, position_ids, sensitive_token_mask, target_classes=None, aggregation=\"L1\"):\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, output_attentions=True)\n",
    "    attentions = outputs.attentions  # L x batch x heads x seq x seq\n",
    "    all_attentions = torch.stack(attentions)  # L x batch x heads x seq x seq\n",
    "    attention_mask_expanded = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "    attention_mask_matrix = attention_mask_expanded * attention_mask_expanded.transpose(-1, -2)\n",
    "    all_attentions = all_attentions * attention_mask_matrix.unsqueeze(0)\n",
    "    # normalize attention weights\n",
    "    attn_weights_sum = all_attentions.sum(dim=-1, keepdim=True) + 1e-9  # Add epsilon to avoid division by zero\n",
    "    all_attentions = all_attentions / attn_weights_sum\n",
    "    # mean over heads, mean over layers\n",
    "    avg_attn_heads = all_attentions.mean(dim=2)\n",
    "    avg_attn = avg_attn_heads.mean(dim=0)\n",
    "    attr = avg_attn[:, 0, :]  # token contributions to [CLS] (position 0)\n",
    "\n",
    "    # take only positions with sensitive token mask\n",
    "    if sensitive_token_mask is None:\n",
    "        sensitive_token_mask = attention_mask.clone().to(attention_mask.device)\n",
    "    attr = attr * sensitive_token_mask\n",
    "\n",
    "    # attr loss: mean over all sensitive token positions\n",
    "    attr_loss = attr.sum()\n",
    "    return attr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f9e444c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0000, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer([text1, text2], return_tensors=\"pt\", padding=True).to(device)\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "position_ids = torch.arange(input_ids.size(1), dtype=torch.long, device=device).unsqueeze(0).repeat(input_ids.size(0), 1)\n",
    "token_type_ids = inputs[\"token_type_ids\"]\n",
    "baseline_token_ids = tokenizer.pad_token_id  # Assuming 'pad' refers to the padding token ID\n",
    "#attr, attr_loss = deeplift_attr(model, input_ids, attention_mask, token_type_ids, position_ids, None, baseline_token_ids, None, \"L2\")\n",
    "attr_loss = raw_attention_attr(model, input_ids, attention_mask, token_type_ids, position_ids, None, None, \"L2\")\n",
    "print(attr_loss)\n",
    "# print(attr)\n",
    "# # compute gradient of attr loss wrt model parameters\n",
    "# model.zero_grad()\n",
    "# attr_loss.backward()\n",
    "\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.grad is not None:\n",
    "#         print(f\"Gradient for {name}:\")\n",
    "#         print(param.grad)\n",
    "#     else:\n",
    "#         print(f\"No gradient computed for {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b72c1941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0196, 0.0218, 0.0300, 0.0121, 0.0042, 0.0097, 0.0063, 0.0116, 0.0378,\n",
      "         0.0121, 0.0608, 0.0115, 0.0114, 0.0235, 0.0090, 0.0124, 0.0235, 0.0092,\n",
      "         0.0111, 0.0079, 0.0044, 0.0058, 0.0049, 0.0225, 0.0057, 0.0034, 0.0107,\n",
      "         0.0086, 0.0055, 0.0052, 0.0023, 0.0040, 0.0049, 0.0155, 0.0062, 0.0038,\n",
      "         0.0034, 0.0053, 0.0063, 0.0139, 0.0329, 0.0076, 0.0129]],\n",
      "       device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(0.5412, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Gradient for model.bert.embeddings.word_embeddings.weight:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Gradient for model.bert.embeddings.position_embeddings.weight:\n",
      "tensor([[ 1.4060e-04, -5.3115e-04, -2.9448e-05,  ...,  1.0426e-05,\n",
      "          4.5507e-05,  3.5155e-05],\n",
      "        [ 7.5782e-04,  7.9464e-04,  7.5394e-05,  ...,  5.7519e-04,\n",
      "          4.1682e-04,  9.0477e-05],\n",
      "        [ 4.8999e-04,  3.2900e-05,  1.1648e-03,  ..., -1.8387e-05,\n",
      "          1.2840e-04, -2.1475e-04],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]], device='cuda:0')\n",
      "Gradient for model.bert.embeddings.token_type_embeddings.weight:\n",
      "tensor([[-0.0003,  0.0033, -0.0160,  ...,  0.0096,  0.0037,  0.0053],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "Gradient for model.bert.embeddings.LayerNorm.weight:\n",
      "tensor([ 0.0004,  0.0004,  0.0013,  0.0003,  0.0009,  0.0016,  0.0009,  0.0003,\n",
      "         0.0007,  0.0007,  0.0001,  0.0009,  0.0010,  0.0009,  0.0004,  0.0010,\n",
      "         0.0004,  0.0005,  0.0028,  0.0010,  0.0004,  0.0004,  0.0009,  0.0005,\n",
      "         0.0010,  0.0010,  0.0006,  0.0013,  0.0003,  0.0003,  0.0004,  0.0005,\n",
      "         0.0005,  0.0006,  0.0011,  0.0006,  0.0004,  0.0004,  0.0006,  0.0009,\n",
      "         0.0012,  0.0004,  0.0019,  0.0010,  0.0025,  0.0011,  0.0008,  0.0004,\n",
      "         0.0007,  0.0006,  0.0005,  0.0002,  0.0014,  0.0031,  0.0003,  0.0004,\n",
      "         0.0003,  0.0004,  0.0005,  0.0005,  0.0015,  0.0007,  0.0010,  0.0004,\n",
      "         0.0004,  0.0005,  0.0009,  0.0026,  0.0009,  0.0005,  0.0006,  0.0003,\n",
      "         0.0006,  0.0004,  0.0003,  0.0006,  0.0003,  0.0013,  0.0003,  0.0004,\n",
      "         0.0007,  0.0002,  0.0007,  0.0006,  0.0003,  0.0008,  0.0002,  0.0004,\n",
      "         0.0016,  0.0004,  0.0006,  0.0004,  0.0008,  0.0002,  0.0006,  0.0009,\n",
      "         0.0021,  0.0002,  0.0006,  0.0005,  0.0007,  0.0014,  0.0006,  0.0008,\n",
      "         0.0006,  0.0020,  0.0011,  0.0014,  0.0005,  0.0008,  0.0004,  0.0008,\n",
      "         0.0004,  0.0017,  0.0015,  0.0003,  0.0011,  0.0003,  0.0023,  0.0008,\n",
      "         0.0010,  0.0031,  0.0006,  0.0007,  0.0012,  0.0003,  0.0004,  0.0004,\n",
      "         0.0003,  0.0005,  0.0004,  0.0006,  0.0011,  0.0003,  0.0005,  0.0009,\n",
      "         0.0003,  0.0002,  0.0004,  0.0095,  0.0004,  0.0006,  0.0005,  0.0008,\n",
      "         0.0012,  0.0187,  0.0002,  0.0007,  0.0010,  0.0005,  0.0007,  0.0006,\n",
      "         0.0009,  0.0003,  0.0003,  0.0013,  0.0009,  0.0008,  0.0010,  0.0085,\n",
      "         0.0007,  0.0010,  0.0005,  0.0006,  0.0014,  0.0002,  0.0009,  0.0008,\n",
      "         0.0049,  0.0010,  0.0015,  0.0006,  0.0013,  0.0003,  0.0009, -0.0010,\n",
      "         0.0012,  0.0010,  0.0003,  0.0003,  0.0005,  0.0005,  0.0007,  0.0002,\n",
      "         0.0005,  0.0014,  0.0009,  0.0008,  0.0005,  0.0009,  0.0002,  0.0008,\n",
      "         0.0004,  0.0003,  0.0007,  0.0003,  0.0005,  0.0002,  0.0003,  0.0004,\n",
      "         0.0010,  0.0006,  0.0007,  0.0003,  0.0004,  0.0002,  0.0004,  0.0005,\n",
      "         0.0020,  0.0004,  0.0005,  0.0003,  0.0004,  0.0004,  0.0004,  0.0021,\n",
      "         0.0043,  0.0010,  0.0010,  0.0007,  0.0008,  0.0004,  0.0005,  0.0005,\n",
      "         0.0014,  0.0022,  0.0006,  0.0004,  0.0011,  0.0003,  0.0006,  0.0008,\n",
      "         0.0005,  0.0004,  0.0017,  0.0003,  0.0007,  0.0003,  0.0008,  0.0018,\n",
      "         0.0012,  0.0008,  0.0005,  0.0005,  0.0009,  0.0021,  0.0009,  0.0005,\n",
      "         0.0003,  0.0006,  0.0004,  0.0003,  0.0028,  0.0006,  0.0011,  0.0017,\n",
      "         0.0006,  0.0002,  0.0006,  0.0009,  0.0005,  0.0002,  0.0007,  0.0003,\n",
      "         0.0008,  0.0007,  0.0008,  0.0011,  0.0013,  0.0025,  0.0019,  0.0007,\n",
      "         0.0007,  0.0004,  0.0008,  0.0005,  0.0008,  0.0009,  0.0008,  0.0003,\n",
      "         0.0008,  0.0028,  0.0003,  0.0010,  0.0005,  0.0011,  0.0005,  0.0009,\n",
      "         0.0009,  0.0010,  0.0015,  0.0009,  0.0004,  0.0001,  0.0006,  0.0005,\n",
      "         0.0013,  0.0004,  0.0004,  0.0007,  0.0005,  0.0002,  0.0002,  0.0005,\n",
      "         0.0016,  0.0015,  0.0004,  0.0004,  0.0538,  0.0005,  0.0002,  0.0019,\n",
      "         0.0003,  0.0003,  0.0004,  0.0009,  0.0004,  0.0008,  0.0012,  0.0024,\n",
      "         0.0006,  0.0007,  0.0003,  0.0008,  0.0007,  0.0010,  0.0006,  0.0011,\n",
      "         0.0003,  0.0016,  0.0004,  0.0009,  0.0007,  0.0004,  0.0008,  0.0012,\n",
      "         0.0006,  0.0006,  0.0010,  0.0004,  0.0003,  0.0014,  0.0003,  0.0011,\n",
      "         0.0007,  0.0022,  0.0007,  0.0004,  0.0006,  0.0018,  0.0016,  0.0016,\n",
      "         0.0010,  0.0021,  0.0003,  0.0007,  0.0005,  0.0005,  0.0005,  0.0011,\n",
      "         0.0009,  0.0007,  0.0006,  0.0002,  0.0003,  0.0007,  0.0006,  0.0005,\n",
      "         0.0011,  0.0004,  0.0006,  0.0004,  0.0007,  0.0010,  0.0005,  0.0005,\n",
      "         0.0002,  0.0003,  0.0010,  0.0006,  0.0024,  0.0046,  0.0013,  0.0003,\n",
      "         0.0006,  0.0005,  0.0004,  0.0009,  0.0012,  0.0003,  0.0008,  0.0003,\n",
      "         0.0010,  0.0005,  0.0003,  0.0025,  0.0015,  0.0003,  0.0003,  0.0010,\n",
      "         0.0004,  0.0012,  0.0020,  0.0005,  0.0004,  0.0004,  0.0009,  0.0011,\n",
      "         0.0016,  0.0006,  0.0007,  0.0008,  0.0003,  0.0003,  0.0005,  0.0005,\n",
      "         0.0004,  0.0009,  0.0003,  0.0026,  0.0018,  0.0008,  0.0005,  0.0004,\n",
      "         0.0011,  0.0008,  0.0008,  0.0002,  0.0028,  0.0008,  0.0007,  0.0003,\n",
      "         0.0002,  0.0006,  0.0008,  0.0005,  0.0008,  0.0002,  0.0008,  0.0007,\n",
      "         0.0002,  0.0013,  0.0009,  0.0007,  0.0011,  0.0006,  0.0006,  0.0002,\n",
      "         0.0006,  0.0003,  0.0009,  0.0003,  0.0010,  0.0005,  0.0008,  0.0015,\n",
      "         0.0017,  0.0007,  0.0003,  0.0005,  0.0003,  0.0032,  0.0006,  0.0002,\n",
      "         0.0007,  0.0004,  0.0010,  0.0007,  0.0004,  0.0004,  0.0013,  0.0002,\n",
      "         0.0003,  0.0005,  0.0007,  0.0004,  0.0003,  0.0010,  0.0008,  0.0006,\n",
      "         0.0006,  0.0012,  0.0014,  0.0003,  0.0002,  0.0006,  0.0002,  0.0003,\n",
      "         0.0003,  0.0006,  0.0005,  0.0007,  0.0008,  0.0005,  0.0006,  0.0006,\n",
      "         0.0011,  0.0008,  0.0010,  0.0005,  0.0008,  0.0009,  0.0005,  0.0006,\n",
      "         0.0003,  0.0011,  0.0003,  0.0009,  0.0004,  0.0031,  0.0006,  0.0002,\n",
      "         0.0005,  0.0021,  0.0009,  0.0006,  0.0009,  0.0027,  0.0004,  0.0009,\n",
      "         0.0005,  0.0009,  0.0009,  0.0008,  0.0004,  0.0014,  0.0014,  0.0016,\n",
      "         0.0009,  0.0178,  0.0006,  0.0006,  0.0009,  0.0004,  0.0007,  0.0004,\n",
      "         0.0010,  0.0005,  0.0017,  0.0008,  0.0004,  0.0113,  0.0009,  0.0007,\n",
      "         0.0003,  0.0048,  0.0005,  0.0003,  0.0005,  0.0009,  0.0004,  0.0010,\n",
      "         0.0007,  0.0005,  0.0005,  0.0003,  0.0005,  0.0007,  0.0008,  0.0006,\n",
      "         0.0003,  0.0023,  0.0002,  0.0009,  0.0010,  0.0002,  0.0004,  0.0002,\n",
      "         0.0034,  0.0002,  0.0007,  0.0007,  0.0014,  0.0005,  0.0008,  0.0013,\n",
      "         0.0006,  0.0013,  0.0018,  0.0007,  0.0007,  0.0003,  0.0006,  0.0006,\n",
      "         0.0004,  0.0007,  0.0007,  0.0007,  0.0005,  0.0017,  0.0008,  0.0005,\n",
      "         0.0004,  0.0004,  0.0006,  0.0007,  0.0005,  0.0005,  0.0006,  0.0005,\n",
      "         0.0004,  0.0022,  0.0008,  0.0006,  0.0008,  0.0004,  0.0008,  0.0004,\n",
      "         0.0003,  0.0002,  0.0003,  0.0004,  0.0008,  0.0006,  0.0030,  0.0004,\n",
      "         0.0003,  0.0004,  0.0012,  0.0006,  0.0002,  0.0007,  0.0002,  0.0006,\n",
      "         0.0007,  0.0005,  0.0005,  0.0005,  0.0004,  0.0019,  0.0006,  0.0004,\n",
      "         0.0009,  0.0005,  0.0004,  0.0004,  0.0003,  0.0004,  0.0008,  0.0007,\n",
      "         0.0020,  0.0004,  0.0005,  0.0003,  0.0003,  0.0014,  0.0005,  0.0006,\n",
      "         0.0022,  0.0009,  0.0008,  0.0006,  0.0007,  0.0013,  0.0005,  0.0010,\n",
      "         0.0008,  0.0005,  0.0005,  0.0006,  0.0012,  0.0009,  0.0004,  0.0004,\n",
      "         0.0007,  0.0008,  0.0011,  0.0004,  0.0011,  0.0009,  0.0013,  0.0004,\n",
      "         0.0007,  0.0002,  0.0009,  0.0001,  0.0004,  0.0004,  0.0003,  0.0004,\n",
      "         0.0007,  0.0004,  0.0006,  0.0004,  0.0009,  0.0007,  0.0010,  0.0004,\n",
      "         0.0004,  0.0006,  0.0011,  0.0010,  0.0008,  0.0005,  0.0030,  0.0006,\n",
      "         0.0004,  0.0005,  0.0013,  0.0014,  0.0004,  0.0004,  0.0007,  0.0012,\n",
      "         0.0009,  0.0005,  0.0007,  0.0005,  0.0013,  0.0014,  0.0008,  0.0002,\n",
      "         0.0011,  0.0007,  0.0013,  0.0008,  0.0003,  0.0004,  0.0009,  0.0006,\n",
      "         0.0004,  0.0009,  0.0010,  0.0012,  0.0007,  0.0007,  0.0003,  0.0004,\n",
      "         0.0005,  0.0007,  0.0006,  0.0019,  0.0002,  0.0012,  0.0012,  0.0010,\n",
      "         0.0008,  0.0004,  0.0008,  0.0009,  0.0008,  0.0003,  0.0007,  0.0018,\n",
      "         0.0007,  0.0014,  0.0003,  0.0006,  0.0008,  0.0011,  0.0016,  0.0005,\n",
      "         0.0016,  0.0005,  0.0007,  0.0007,  0.0004,  0.0012,  0.0007,  0.0012,\n",
      "         0.0014,  0.0002,  0.0007,  0.0008,  0.0005,  0.0008,  0.0005,  0.0006],\n",
      "       device='cuda:0')\n",
      "Gradient for model.bert.embeddings.LayerNorm.bias:\n",
      "tensor([ 5.4715e-06,  3.5226e-04, -1.1406e-03, -1.6736e-04, -7.5400e-04,\n",
      "        -4.6077e-05,  1.4183e-03,  1.8314e-04, -3.6303e-04, -4.4354e-05,\n",
      "         1.5191e-04,  1.0057e-04, -4.6431e-05,  4.6388e-04,  2.5608e-04,\n",
      "        -5.4647e-04,  4.1424e-04,  4.6006e-05, -6.6812e-04, -3.9442e-04,\n",
      "        -3.4725e-04,  5.1616e-05, -4.4101e-04, -3.6186e-05,  6.5600e-04,\n",
      "        -9.3822e-04,  2.1678e-04,  1.2769e-03,  2.0196e-04,  1.0631e-04,\n",
      "        -4.6674e-05,  3.8483e-04, -1.6022e-04, -3.9705e-04, -7.0523e-04,\n",
      "         9.9338e-05,  5.5731e-04,  3.7551e-06,  4.4834e-04,  4.9032e-05,\n",
      "         3.5201e-04,  6.7574e-05,  8.5668e-04,  6.2216e-04,  8.3517e-04,\n",
      "         3.9552e-04, -1.1568e-04,  2.6341e-04,  3.2537e-04,  6.5427e-04,\n",
      "        -5.1828e-04, -8.5036e-05, -5.4930e-04,  7.2087e-04,  1.0506e-04,\n",
      "        -1.1865e-04, -1.9703e-04,  3.9796e-05,  3.8323e-04, -1.6672e-04,\n",
      "        -1.0298e-03, -2.8085e-04, -5.1630e-04, -8.0336e-05, -8.7873e-05,\n",
      "        -1.9880e-04,  5.1356e-04, -5.1489e-04,  2.8750e-04,  6.2931e-04,\n",
      "        -3.9787e-04,  8.5597e-05, -3.3084e-04,  4.6909e-04,  1.9841e-04,\n",
      "        -9.3653e-05,  2.5683e-04,  3.8128e-04,  1.0672e-04,  3.5753e-04,\n",
      "         7.1934e-05, -2.1366e-06, -5.3046e-04, -7.8127e-04, -1.8696e-04,\n",
      "         2.9991e-04, -4.1541e-05, -6.1995e-05,  1.0060e-03,  1.6088e-04,\n",
      "         1.5474e-04,  2.8001e-04, -2.8717e-05,  1.3928e-04, -4.9052e-04,\n",
      "        -1.7421e-04, -7.1318e-04,  3.0156e-04, -3.5682e-04, -3.9503e-05,\n",
      "         4.1450e-04, -1.0003e-03,  3.9479e-04,  4.5259e-04, -5.8330e-04,\n",
      "         4.8114e-04, -8.5972e-04,  1.1005e-03,  5.9329e-05, -1.8822e-04,\n",
      "         3.1776e-04,  3.8584e-04, -3.3712e-04,  1.7617e-03, -8.5757e-04,\n",
      "        -1.8974e-04,  8.2931e-04,  3.2111e-04, -7.3820e-04, -2.0396e-04,\n",
      "        -4.4340e-04,  1.1542e-03,  2.5834e-04,  1.3843e-04,  4.6216e-04,\n",
      "        -3.4158e-04, -2.5106e-04,  5.2070e-04,  2.8444e-04,  3.2164e-04,\n",
      "         3.9532e-05, -7.5840e-05,  8.7581e-05, -8.0004e-05, -4.9480e-04,\n",
      "        -1.6184e-04,  1.3555e-04, -1.6574e-06, -1.2051e-04,  2.0097e-04,\n",
      "         1.4681e-04,  1.4291e-04, -3.4209e-04, -5.6996e-04, -3.6270e-04,\n",
      "        -1.6464e-03,  2.9149e-04,  5.3819e-04, -5.9965e-04, -1.5859e-04,\n",
      "         2.7620e-04, -9.8227e-05, -3.9326e-04, -5.6846e-05, -2.6349e-05,\n",
      "        -4.8517e-04, -1.1945e-04,  7.6875e-04, -5.1067e-04,  1.1281e-03,\n",
      "         3.1941e-04,  8.7237e-04,  2.2897e-04,  1.1276e-04,  1.1087e-04,\n",
      "         1.7482e-04, -5.1502e-06,  2.1798e-04, -2.0929e-04, -5.7971e-04,\n",
      "        -3.1382e-04, -5.5578e-04,  1.0332e-03,  3.4820e-04,  1.2826e-04,\n",
      "        -1.0118e-03, -6.0776e-04,  6.1110e-04, -2.7290e-04, -2.6955e-04,\n",
      "         3.5200e-04,  1.3480e-04, -1.4190e-04,  3.6864e-05,  3.5579e-04,\n",
      "         1.1781e-03, -1.5461e-04,  2.6058e-04, -1.7023e-04, -1.3421e-04,\n",
      "        -8.0823e-06,  6.7938e-04, -1.7143e-04, -1.1896e-04, -2.9662e-04,\n",
      "        -3.3447e-05,  8.4287e-05,  2.9989e-04,  2.8412e-04,  4.2093e-05,\n",
      "         2.5774e-05,  5.0218e-04,  3.9117e-04,  1.1614e-04,  2.1936e-04,\n",
      "        -1.4302e-05, -2.3569e-04, -8.7038e-05, -6.3417e-04,  2.2955e-04,\n",
      "         1.3333e-04, -1.4712e-04, -3.7691e-04, -7.2386e-05,  9.2248e-05,\n",
      "         4.6083e-05, -1.6111e-03, -5.5377e-04, -3.1939e-04,  3.3151e-04,\n",
      "         4.9025e-04, -3.0552e-04, -1.3411e-04,  4.7723e-04, -1.3065e-03,\n",
      "         6.3815e-04,  5.5436e-04,  2.6969e-04, -4.7767e-04,  1.6458e-04,\n",
      "         4.4100e-04,  4.3019e-05, -3.0645e-04, -1.9189e-04, -1.2477e-03,\n",
      "         8.2565e-05, -5.8030e-04, -3.1180e-04,  5.1325e-04,  1.5011e-03,\n",
      "        -4.7283e-04, -4.9977e-04,  2.9305e-04,  3.9358e-04, -3.8681e-04,\n",
      "         2.4371e-04, -5.4771e-05, -2.4677e-04, -7.7794e-05, -2.0792e-04,\n",
      "         1.7817e-04,  1.9615e-04,  1.0959e-03,  2.0651e-04, -8.5836e-05,\n",
      "         3.2617e-04, -1.8225e-04, -1.2370e-04,  2.9540e-05,  5.1846e-04,\n",
      "        -1.5668e-04, -1.0105e-04,  8.8613e-06,  2.9391e-04, -5.0573e-05,\n",
      "         4.5650e-04,  2.4735e-04,  7.1801e-04,  1.2102e-04, -1.2947e-03,\n",
      "         1.0652e-03, -1.9899e-04,  5.3247e-04,  1.3766e-04, -4.8556e-04,\n",
      "        -3.0372e-05, -1.8239e-04, -4.7258e-05,  5.4926e-05, -7.8833e-05,\n",
      "        -7.5437e-04, -9.6833e-04,  1.8896e-04, -6.8154e-05, -5.5955e-04,\n",
      "        -8.1020e-04,  2.5574e-04, -2.8062e-04,  1.0371e-04,  6.9842e-05,\n",
      "         1.6051e-04,  5.4614e-05, -3.7260e-04, -4.4129e-05,  3.9143e-05,\n",
      "        -4.0734e-05, -3.7808e-04,  4.1621e-04,  1.9457e-04, -2.4447e-04,\n",
      "         5.8829e-04,  3.1431e-05,  1.1296e-04,  1.7823e-05,  1.1098e-03,\n",
      "        -3.1037e-04,  2.3209e-04,  8.6032e-05, -7.3856e-03,  4.3905e-04,\n",
      "         7.8946e-06, -5.4791e-04, -3.0596e-04,  8.4005e-05,  2.6363e-04,\n",
      "         3.1814e-04,  2.6592e-04, -7.8387e-04, -8.0086e-04,  4.8897e-04,\n",
      "         2.3217e-04, -4.6543e-04, -2.4061e-05, -4.0975e-04, -1.1456e-04,\n",
      "         2.1351e-04, -8.0964e-04, -1.5517e-03, -1.3328e-04, -7.7566e-04,\n",
      "        -6.6495e-04,  6.0725e-05, -3.4544e-04,  4.0357e-05,  6.3099e-04,\n",
      "        -2.9285e-05,  1.1347e-04,  5.5180e-05, -5.6012e-05, -2.3625e-04,\n",
      "         2.7661e-04,  1.0987e-03, -2.2829e-04,  1.0870e-03, -6.3529e-04,\n",
      "        -1.6395e-04, -2.8894e-04, -1.8397e-04,  7.1438e-04,  5.2625e-04,\n",
      "        -7.9098e-04,  3.8769e-04, -4.4918e-04,  5.4469e-04,  2.2937e-04,\n",
      "        -4.9270e-04,  5.4198e-04, -2.0458e-04, -1.0317e-04, -6.9054e-04,\n",
      "        -1.9261e-04, -2.6171e-04, -4.4244e-05, -2.0150e-05, -1.9198e-05,\n",
      "         7.6420e-04, -1.1989e-04,  1.4427e-04,  1.1511e-05, -3.8652e-04,\n",
      "         3.9397e-04,  4.6026e-04,  5.6154e-05,  8.1697e-05,  3.2804e-05,\n",
      "         7.1571e-05, -9.3868e-05,  1.1365e-04,  2.0933e-04, -1.6725e-04,\n",
      "         1.5938e-03, -9.9179e-04,  6.4058e-04,  1.4838e-04,  1.4588e-04,\n",
      "        -1.2171e-04,  1.8867e-04,  1.2782e-05, -2.7535e-04, -8.5647e-06,\n",
      "        -5.7826e-04, -1.9405e-04, -1.0850e-04, -4.1401e-04,  7.6269e-05,\n",
      "        -6.2256e-04,  6.9876e-04,  1.4972e-05, -9.8428e-05,  3.1626e-04,\n",
      "         4.1446e-06,  3.4226e-04, -1.3254e-03,  1.2684e-04, -6.2596e-05,\n",
      "         7.1419e-05, -8.0969e-05,  2.8993e-04,  1.8797e-04,  4.7835e-04,\n",
      "         6.6798e-04, -2.8274e-04,  2.0160e-04, -5.4595e-05,  3.0242e-04,\n",
      "         6.1676e-05,  2.9100e-04,  4.9410e-05, -3.4033e-04,  1.1521e-03,\n",
      "        -9.3401e-05, -6.2746e-04,  2.7030e-04, -1.5135e-04, -6.4505e-05,\n",
      "         2.1373e-04,  2.0741e-04,  9.1000e-05, -4.1156e-04,  6.1814e-04,\n",
      "        -3.9203e-04, -1.5091e-04, -8.7968e-05, -1.5749e-04,  9.3961e-04,\n",
      "        -3.0711e-05, -7.0234e-05,  1.6217e-04, -7.1551e-04,  4.6046e-04,\n",
      "        -7.5431e-05, -8.9816e-04,  3.8896e-04, -7.9840e-05,  7.1897e-04,\n",
      "         4.2782e-04, -8.4369e-05, -1.1421e-04, -5.8560e-05,  1.2241e-04,\n",
      "         3.3194e-04, -7.3591e-05, -6.3093e-04,  2.6472e-04, -4.5960e-04,\n",
      "        -1.1396e-03, -4.5062e-04,  6.0128e-04,  1.4737e-04, -2.2467e-04,\n",
      "        -2.1420e-04, -4.0510e-04,  1.1198e-04,  1.4620e-05,  6.8378e-04,\n",
      "         4.9337e-04,  2.6744e-04, -5.1108e-04, -1.4853e-04,  3.1679e-05,\n",
      "        -3.5048e-04,  1.6456e-04,  2.9098e-04, -4.7884e-06, -2.1829e-04,\n",
      "        -1.0006e-04, -1.6696e-04, -4.5229e-04,  3.5764e-04,  5.4345e-04,\n",
      "         4.7120e-04, -2.2207e-04,  1.9210e-04, -2.4564e-04, -1.5435e-04,\n",
      "         1.0345e-04, -2.5964e-05, -2.0243e-04,  1.4479e-04, -1.1339e-04,\n",
      "        -1.3909e-04, -2.7209e-04, -5.4668e-05,  1.5945e-04, -4.1279e-04,\n",
      "        -5.8036e-04,  2.1761e-04, -7.9641e-04,  5.3110e-04,  1.6486e-04,\n",
      "         8.7867e-04,  7.4227e-04,  6.5297e-04, -4.7779e-04, -1.7180e-04,\n",
      "         2.8896e-04,  1.4033e-04, -3.5492e-04,  1.0845e-04, -1.3020e-03,\n",
      "         4.0716e-04, -4.4500e-05,  3.0232e-04, -1.0463e-03, -6.4355e-04,\n",
      "         1.4064e-04, -5.4722e-04,  1.0634e-03, -2.2530e-04,  7.6625e-04,\n",
      "        -1.0290e-04,  4.6343e-04, -8.6274e-04, -5.4000e-04, -3.5464e-04,\n",
      "        -6.8228e-04, -1.4409e-03, -1.0743e-03, -4.5731e-04, -1.5265e-03,\n",
      "         1.4105e-04, -2.4466e-05, -4.9867e-04,  1.1953e-04, -3.8107e-04,\n",
      "         2.2420e-04,  2.2898e-04,  4.1955e-04,  1.3367e-04, -2.5070e-04,\n",
      "        -8.3156e-05, -7.5921e-04,  7.0351e-04, -4.0027e-04,  3.5936e-04,\n",
      "         4.2592e-04,  2.4685e-04,  3.2439e-04,  3.8479e-04,  6.1870e-05,\n",
      "        -1.7693e-04, -8.8163e-04, -1.8429e-05,  3.3283e-04, -3.3993e-04,\n",
      "        -5.5830e-05,  1.6836e-04,  4.5867e-04, -4.7391e-05, -4.8911e-05,\n",
      "         1.5352e-04, -2.1604e-03, -9.1415e-05, -4.6078e-04, -1.4114e-04,\n",
      "        -8.7777e-05, -2.4428e-04,  1.7966e-04, -1.8301e-03, -1.5491e-04,\n",
      "         3.5493e-04,  5.8891e-05, -1.0138e-03, -2.8199e-04,  4.9326e-05,\n",
      "         6.6685e-04,  1.0130e-04, -5.5111e-04, -6.2998e-04,  2.6300e-04,\n",
      "        -4.9475e-05, -4.0953e-04,  1.7639e-04, -6.0417e-04,  3.7865e-05,\n",
      "         2.9082e-04,  1.3707e-04,  2.8468e-04, -1.3592e-04, -9.0423e-04,\n",
      "        -3.2164e-04,  4.3306e-05, -7.6883e-05, -1.5373e-04, -6.6181e-04,\n",
      "         3.1523e-04, -3.9178e-04, -2.1930e-05, -5.4986e-05,  1.5695e-04,\n",
      "        -4.8173e-05, -1.4183e-03, -1.5613e-04, -1.6370e-04,  3.5985e-04,\n",
      "        -1.5490e-04,  4.7044e-04, -5.2306e-05,  1.4169e-04, -2.3407e-04,\n",
      "         2.8962e-04, -5.7394e-05, -5.1923e-05, -1.5925e-04, -1.0874e-03,\n",
      "        -5.4362e-06,  1.5288e-04,  4.8000e-04, -6.6608e-04, -1.5409e-04,\n",
      "         5.0721e-05, -4.3850e-06,  1.1343e-04, -4.7469e-04,  4.0974e-05,\n",
      "         4.0252e-05, -1.1295e-04, -5.2312e-04, -1.4788e-04,  6.3485e-04,\n",
      "         2.9888e-04, -1.2208e-04, -4.6196e-04, -8.6776e-05,  6.8260e-05,\n",
      "        -1.4854e-04, -3.1730e-05,  6.3795e-04, -8.0086e-05,  1.0100e-03,\n",
      "        -1.0413e-03, -1.8322e-04,  7.5590e-05, -5.3248e-05, -2.0382e-04,\n",
      "         7.9746e-04, -8.3462e-05, -5.0317e-05, -2.0597e-03,  2.2008e-04,\n",
      "         4.8490e-04,  1.5581e-04, -9.0325e-05,  3.7470e-04,  6.5952e-04,\n",
      "         4.5376e-04, -4.9812e-05,  2.7134e-04, -4.6563e-04,  1.9284e-04,\n",
      "        -1.3862e-04,  3.2643e-04,  5.8910e-04, -2.3159e-04, -3.1888e-04,\n",
      "        -1.6225e-04, -7.0528e-04,  2.2680e-05, -3.2776e-04,  9.7780e-04,\n",
      "        -3.6296e-04,  8.4202e-05, -1.7559e-04, -6.7273e-05, -2.4883e-04,\n",
      "         9.7751e-05, -8.6167e-05, -2.1536e-05,  2.5743e-04, -4.1124e-04,\n",
      "         5.3739e-04, -1.2723e-04, -2.0939e-04,  9.1706e-05,  1.0218e-03,\n",
      "         1.0777e-04, -4.5109e-04,  1.3927e-04,  3.4072e-04, -5.1109e-04,\n",
      "         1.9167e-04, -7.5889e-04, -7.4992e-05, -1.8902e-04, -1.5616e-03,\n",
      "         2.5128e-04, -1.8640e-04,  8.4521e-06, -9.7701e-04, -1.3312e-04,\n",
      "         1.6032e-04, -1.7503e-04,  1.8140e-04,  6.8476e-04, -2.8557e-04,\n",
      "         3.2106e-04, -1.1738e-04,  1.2463e-04,  1.1046e-03,  1.2529e-03,\n",
      "        -2.5442e-04,  1.1786e-04, -3.6401e-04,  6.8823e-04, -1.3284e-03,\n",
      "         4.7534e-04,  1.9314e-05, -1.3085e-04, -2.0183e-04, -3.4717e-04,\n",
      "         4.3262e-04,  4.5827e-04,  3.3873e-04,  6.5090e-04,  6.8759e-04,\n",
      "         3.0524e-04, -1.1112e-04, -1.5645e-04, -2.6369e-04, -6.5550e-04,\n",
      "         9.0498e-05,  2.8287e-04,  1.2916e-04, -8.1578e-04, -2.7529e-05,\n",
      "        -3.3002e-04, -1.5340e-04, -1.9529e-04,  4.5018e-04,  4.6395e-04,\n",
      "        -4.4278e-04,  1.8040e-04,  2.2498e-05,  8.0141e-04,  2.2723e-05,\n",
      "        -4.4424e-04,  1.0363e-04,  1.9735e-04, -7.8082e-04, -2.6682e-04,\n",
      "         1.1747e-03, -2.5082e-04, -8.1952e-04,  2.5795e-04, -2.8016e-04,\n",
      "        -5.2192e-04, -1.7937e-05,  7.9968e-04, -4.9335e-04,  7.7286e-04,\n",
      "        -1.2246e-03, -2.2575e-05,  4.9997e-04, -3.9246e-04,  1.4359e-04,\n",
      "         8.2826e-04,  3.5064e-04,  2.5404e-04], device='cuda:0')\n",
      "No gradient computed for model.bert.encoder.layer.0.attention.self.query.weight\n",
      "No gradient computed for model.bert.encoder.layer.0.attention.self.query.bias\n",
      "No gradient computed for model.bert.encoder.layer.0.attention.self.key.weight\n",
      "No gradient computed for model.bert.encoder.layer.0.attention.self.key.bias\n",
      "No gradient computed for model.bert.encoder.layer.0.attention.self.value.weight\n",
      "No gradient computed for model.bert.encoder.layer.0.attention.self.value.bias\n",
      "No gradient computed for model.bert.encoder.layer.0.attention.output.dense.weight\n",
      "No gradient computed for model.bert.encoder.layer.0.attention.output.dense.bias\n",
      "No gradient computed for model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "No gradient computed for model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "No gradient computed for model.bert.encoder.layer.0.intermediate.dense.weight\n",
      "No gradient computed for model.bert.encoder.layer.0.intermediate.dense.bias\n",
      "No gradient computed for model.bert.encoder.layer.0.output.dense.weight\n",
      "No gradient computed for model.bert.encoder.layer.0.output.dense.bias\n",
      "No gradient computed for model.bert.encoder.layer.0.output.LayerNorm.weight\n",
      "No gradient computed for model.bert.encoder.layer.0.output.LayerNorm.bias\n",
      "No gradient computed for model.bert.encoder.layer.1.attention.self.query.weight\n",
      "No gradient computed for model.bert.encoder.layer.1.attention.self.query.bias\n",
      "No gradient computed for model.bert.encoder.layer.1.attention.self.key.weight\n",
      "No gradient computed for model.bert.encoder.layer.1.attention.self.key.bias\n",
      "No gradient computed for model.bert.encoder.layer.1.attention.self.value.weight\n",
      "No gradient computed for model.bert.encoder.layer.1.attention.self.value.bias\n",
      "No gradient computed for model.bert.encoder.layer.1.attention.output.dense.weight\n",
      "No gradient computed for model.bert.encoder.layer.1.attention.output.dense.bias\n",
      "No gradient computed for model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "No gradient computed for model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "No gradient computed for model.bert.encoder.layer.1.intermediate.dense.weight\n",
      "No gradient computed for model.bert.encoder.layer.1.intermediate.dense.bias\n",
      "No gradient computed for model.bert.encoder.layer.1.output.dense.weight\n",
      "No gradient computed for model.bert.encoder.layer.1.output.dense.bias\n",
      "No gradient computed for model.bert.encoder.layer.1.output.LayerNorm.weight\n",
      "No gradient computed for model.bert.encoder.layer.1.output.LayerNorm.bias\n",
      "No gradient computed for model.bert.encoder.layer.2.attention.self.query.weight\n",
      "No gradient computed for model.bert.encoder.layer.2.attention.self.query.bias\n",
      "No gradient computed for model.bert.encoder.layer.2.attention.self.key.weight\n",
      "No gradient computed for model.bert.encoder.layer.2.attention.self.key.bias\n",
      "No gradient computed for model.bert.encoder.layer.2.attention.self.value.weight\n",
      "No gradient computed for model.bert.encoder.layer.2.attention.self.value.bias\n",
      "No gradient computed for model.bert.encoder.layer.2.attention.output.dense.weight\n",
      "No gradient computed for model.bert.encoder.layer.2.attention.output.dense.bias\n",
      "No gradient computed for model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "No gradient computed for model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "No gradient computed for model.bert.encoder.layer.2.intermediate.dense.weight\n",
      "No gradient computed for model.bert.encoder.layer.2.intermediate.dense.bias\n",
      "No gradient computed for model.bert.encoder.layer.2.output.dense.weight\n",
      "No gradient computed for model.bert.encoder.layer.2.output.dense.bias\n",
      "No gradient computed for model.bert.encoder.layer.2.output.LayerNorm.weight\n",
      "No gradient computed for model.bert.encoder.layer.2.output.LayerNorm.bias\n",
      "No gradient computed for model.bert.encoder.layer.3.attention.self.query.weight\n",
      "No gradient computed for model.bert.encoder.layer.3.attention.self.query.bias\n",
      "No gradient computed for model.bert.encoder.layer.3.attention.self.key.weight\n",
      "No gradient computed for model.bert.encoder.layer.3.attention.self.key.bias\n",
      "No gradient computed for model.bert.encoder.layer.3.attention.self.value.weight\n",
      "No gradient computed for model.bert.encoder.layer.3.attention.self.value.bias\n",
      "No gradient computed for model.bert.encoder.layer.3.attention.output.dense.weight\n",
      "No gradient computed for model.bert.encoder.layer.3.attention.output.dense.bias\n",
      "No gradient computed for model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "No gradient computed for model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "No gradient computed for model.bert.encoder.layer.3.intermediate.dense.weight\n",
      "No gradient computed for model.bert.encoder.layer.3.intermediate.dense.bias\n",
      "No gradient computed for model.bert.encoder.layer.3.output.dense.weight\n",
      "No gradient computed for model.bert.encoder.layer.3.output.dense.bias\n",
      "No gradient computed for model.bert.encoder.layer.3.output.LayerNorm.weight\n",
      "No gradient computed for model.bert.encoder.layer.3.output.LayerNorm.bias\n",
      "No gradient computed for model.bert.encoder.layer.4.attention.self.query.weight\n",
      "No gradient computed for model.bert.encoder.layer.4.attention.self.query.bias\n",
      "No gradient computed for model.bert.encoder.layer.4.attention.self.key.weight\n",
      "No gradient computed for model.bert.encoder.layer.4.attention.self.key.bias\n",
      "No gradient computed for model.bert.encoder.layer.4.attention.self.value.weight\n",
      "No gradient computed for model.bert.encoder.layer.4.attention.self.value.bias\n",
      "No gradient computed for model.bert.encoder.layer.4.attention.output.dense.weight\n",
      "No gradient computed for model.bert.encoder.layer.4.attention.output.dense.bias\n",
      "No gradient computed for model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "No gradient computed for model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "No gradient computed for model.bert.encoder.layer.4.intermediate.dense.weight\n",
      "No gradient computed for model.bert.encoder.layer.4.intermediate.dense.bias\n",
      "No gradient computed for model.bert.encoder.layer.4.output.dense.weight\n",
      "No gradient computed for model.bert.encoder.layer.4.output.dense.bias\n",
      "No gradient computed for model.bert.encoder.layer.4.output.LayerNorm.weight\n",
      "No gradient computed for model.bert.encoder.layer.4.output.LayerNorm.bias\n",
      "No gradient computed for model.bert.encoder.layer.5.attention.self.query.weight\n",
      "No gradient computed for model.bert.encoder.layer.5.attention.self.query.bias\n",
      "No gradient computed for model.bert.encoder.layer.5.attention.self.key.weight\n",
      "No gradient computed for model.bert.encoder.layer.5.attention.self.key.bias\n",
      "No gradient computed for model.bert.encoder.layer.5.attention.self.value.weight\n",
      "No gradient computed for model.bert.encoder.layer.5.attention.self.value.bias\n",
      "No gradient computed for model.bert.encoder.layer.5.attention.output.dense.weight\n",
      "No gradient computed for model.bert.encoder.layer.5.attention.output.dense.bias\n",
      "No gradient computed for model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "No gradient computed for model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "No gradient computed for model.bert.encoder.layer.5.intermediate.dense.weight\n",
      "No gradient computed for model.bert.encoder.layer.5.intermediate.dense.bias\n",
      "No gradient computed for model.bert.encoder.layer.5.output.dense.weight\n",
      "No gradient computed for model.bert.encoder.layer.5.output.dense.bias\n",
      "No gradient computed for model.bert.encoder.layer.5.output.LayerNorm.weight\n",
      "No gradient computed for model.bert.encoder.layer.5.output.LayerNorm.bias\n",
      "No gradient computed for model.bert.encoder.layer.6.attention.self.query.weight\n",
      "No gradient computed for model.bert.encoder.layer.6.attention.self.query.bias\n",
      "No gradient computed for model.bert.encoder.layer.6.attention.self.key.weight\n",
      "No gradient computed for model.bert.encoder.layer.6.attention.self.key.bias\n",
      "No gradient computed for model.bert.encoder.layer.6.attention.self.value.weight\n",
      "No gradient computed for model.bert.encoder.layer.6.attention.self.value.bias\n",
      "No gradient computed for model.bert.encoder.layer.6.attention.output.dense.weight\n",
      "No gradient computed for model.bert.encoder.layer.6.attention.output.dense.bias\n",
      "No gradient computed for model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "No gradient computed for model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "No gradient computed for model.bert.encoder.layer.6.intermediate.dense.weight\n",
      "No gradient computed for model.bert.encoder.layer.6.intermediate.dense.bias\n",
      "No gradient computed for model.bert.encoder.layer.6.output.dense.weight\n",
      "No gradient computed for model.bert.encoder.layer.6.output.dense.bias\n",
      "No gradient computed for model.bert.encoder.layer.6.output.LayerNorm.weight\n",
      "No gradient computed for model.bert.encoder.layer.6.output.LayerNorm.bias\n",
      "No gradient computed for model.bert.encoder.layer.7.attention.self.query.weight\n",
      "No gradient computed for model.bert.encoder.layer.7.attention.self.query.bias\n",
      "No gradient computed for model.bert.encoder.layer.7.attention.self.key.weight\n",
      "No gradient computed for model.bert.encoder.layer.7.attention.self.key.bias\n",
      "No gradient computed for model.bert.encoder.layer.7.attention.self.value.weight\n",
      "No gradient computed for model.bert.encoder.layer.7.attention.self.value.bias\n",
      "No gradient computed for model.bert.encoder.layer.7.attention.output.dense.weight\n",
      "No gradient computed for model.bert.encoder.layer.7.attention.output.dense.bias\n",
      "No gradient computed for model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "No gradient computed for model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "No gradient computed for model.bert.encoder.layer.7.intermediate.dense.weight\n",
      "No gradient computed for model.bert.encoder.layer.7.intermediate.dense.bias\n",
      "No gradient computed for model.bert.encoder.layer.7.output.dense.weight\n",
      "No gradient computed for model.bert.encoder.layer.7.output.dense.bias\n",
      "No gradient computed for model.bert.encoder.layer.7.output.LayerNorm.weight\n",
      "No gradient computed for model.bert.encoder.layer.7.output.LayerNorm.bias\n",
      "No gradient computed for model.bert.encoder.layer.8.attention.self.query.weight\n",
      "No gradient computed for model.bert.encoder.layer.8.attention.self.query.bias\n",
      "No gradient computed for model.bert.encoder.layer.8.attention.self.key.weight\n",
      "No gradient computed for model.bert.encoder.layer.8.attention.self.key.bias\n",
      "No gradient computed for model.bert.encoder.layer.8.attention.self.value.weight\n",
      "No gradient computed for model.bert.encoder.layer.8.attention.self.value.bias\n",
      "No gradient computed for model.bert.encoder.layer.8.attention.output.dense.weight\n",
      "No gradient computed for model.bert.encoder.layer.8.attention.output.dense.bias\n",
      "No gradient computed for model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "No gradient computed for model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "No gradient computed for model.bert.encoder.layer.8.intermediate.dense.weight\n",
      "No gradient computed for model.bert.encoder.layer.8.intermediate.dense.bias\n",
      "No gradient computed for model.bert.encoder.layer.8.output.dense.weight\n",
      "No gradient computed for model.bert.encoder.layer.8.output.dense.bias\n",
      "No gradient computed for model.bert.encoder.layer.8.output.LayerNorm.weight\n",
      "No gradient computed for model.bert.encoder.layer.8.output.LayerNorm.bias\n",
      "No gradient computed for model.bert.encoder.layer.9.attention.self.query.weight\n",
      "No gradient computed for model.bert.encoder.layer.9.attention.self.query.bias\n",
      "No gradient computed for model.bert.encoder.layer.9.attention.self.key.weight\n",
      "No gradient computed for model.bert.encoder.layer.9.attention.self.key.bias\n",
      "No gradient computed for model.bert.encoder.layer.9.attention.self.value.weight\n",
      "No gradient computed for model.bert.encoder.layer.9.attention.self.value.bias\n",
      "No gradient computed for model.bert.encoder.layer.9.attention.output.dense.weight\n",
      "No gradient computed for model.bert.encoder.layer.9.attention.output.dense.bias\n",
      "No gradient computed for model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "No gradient computed for model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "No gradient computed for model.bert.encoder.layer.9.intermediate.dense.weight\n",
      "No gradient computed for model.bert.encoder.layer.9.intermediate.dense.bias\n",
      "No gradient computed for model.bert.encoder.layer.9.output.dense.weight\n",
      "No gradient computed for model.bert.encoder.layer.9.output.dense.bias\n",
      "No gradient computed for model.bert.encoder.layer.9.output.LayerNorm.weight\n",
      "No gradient computed for model.bert.encoder.layer.9.output.LayerNorm.bias\n",
      "No gradient computed for model.bert.encoder.layer.10.attention.self.query.weight\n",
      "No gradient computed for model.bert.encoder.layer.10.attention.self.query.bias\n",
      "No gradient computed for model.bert.encoder.layer.10.attention.self.key.weight\n",
      "No gradient computed for model.bert.encoder.layer.10.attention.self.key.bias\n",
      "No gradient computed for model.bert.encoder.layer.10.attention.self.value.weight\n",
      "No gradient computed for model.bert.encoder.layer.10.attention.self.value.bias\n",
      "No gradient computed for model.bert.encoder.layer.10.attention.output.dense.weight\n",
      "No gradient computed for model.bert.encoder.layer.10.attention.output.dense.bias\n",
      "No gradient computed for model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "No gradient computed for model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "No gradient computed for model.bert.encoder.layer.10.intermediate.dense.weight\n",
      "No gradient computed for model.bert.encoder.layer.10.intermediate.dense.bias\n",
      "No gradient computed for model.bert.encoder.layer.10.output.dense.weight\n",
      "No gradient computed for model.bert.encoder.layer.10.output.dense.bias\n",
      "No gradient computed for model.bert.encoder.layer.10.output.LayerNorm.weight\n",
      "No gradient computed for model.bert.encoder.layer.10.output.LayerNorm.bias\n",
      "No gradient computed for model.bert.encoder.layer.11.attention.self.query.weight\n",
      "No gradient computed for model.bert.encoder.layer.11.attention.self.query.bias\n",
      "No gradient computed for model.bert.encoder.layer.11.attention.self.key.weight\n",
      "No gradient computed for model.bert.encoder.layer.11.attention.self.key.bias\n",
      "No gradient computed for model.bert.encoder.layer.11.attention.self.value.weight\n",
      "No gradient computed for model.bert.encoder.layer.11.attention.self.value.bias\n",
      "No gradient computed for model.bert.encoder.layer.11.attention.output.dense.weight\n",
      "No gradient computed for model.bert.encoder.layer.11.attention.output.dense.bias\n",
      "No gradient computed for model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "No gradient computed for model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "No gradient computed for model.bert.encoder.layer.11.intermediate.dense.weight\n",
      "No gradient computed for model.bert.encoder.layer.11.intermediate.dense.bias\n",
      "No gradient computed for model.bert.encoder.layer.11.output.dense.weight\n",
      "No gradient computed for model.bert.encoder.layer.11.output.dense.bias\n",
      "No gradient computed for model.bert.encoder.layer.11.output.LayerNorm.weight\n",
      "No gradient computed for model.bert.encoder.layer.11.output.LayerNorm.bias\n",
      "No gradient computed for model.bert.pooler.dense.weight\n",
      "No gradient computed for model.bert.pooler.dense.bias\n",
      "No gradient computed for model.classifier.weight\n",
      "No gradient computed for model.classifier.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/yifwang/anaconda3/envs/bcos/lib/python3.9/site-packages/transformers/modeling_utils.py:1141: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from captum.attr import DeepLift, IntegratedGradients, Saliency, InputXGradient, KernelShap\n",
    "\n",
    "class BertEmbeddingModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(BertEmbeddingModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, embeddings, attention_mask=None):\n",
    "\n",
    "        head_mask = self.model.get_head_mask(None, self.model.config.num_hidden_layers)\n",
    "        #head_mask = [None] * self.model.config.num_hidden_layers\n",
    "\n",
    "        if hasattr(self.model, \"distilbert\"):\n",
    "            encoder_outputs = self.model.distilbert.transformer(\n",
    "                embeddings,\n",
    "                attn_mask=attention_mask,\n",
    "                head_mask=head_mask,\n",
    "            )\n",
    "            hidden_state = encoder_outputs[0]\n",
    "            pooled_output = hidden_state[:, 0]\n",
    "            pooled_output = self.model.pre_classifier(pooled_output)\n",
    "            if not hasattr(self.model, \"bcos\") or not self.model.bcos:\n",
    "                pooled_output = torch.nn.ReLU()(pooled_output)\n",
    "            pooled_output = self.model.dropout(pooled_output) \n",
    "            logits = self.model.classifier(pooled_output)\n",
    "\n",
    "        elif hasattr(self.model, \"roberta\"):\n",
    "            extended_attention_mask = self.model.get_extended_attention_mask(\n",
    "                attention_mask, embeddings.shape[:2],\n",
    "            )\n",
    "\n",
    "            encoder_outputs = self.model.roberta.encoder(\n",
    "                embeddings,\n",
    "                attention_mask=extended_attention_mask,\n",
    "                head_mask=head_mask,\n",
    "            )\n",
    "            sequence_output = encoder_outputs[0]\n",
    "            #sequence_output = self.model.roberta.pooler(sequence_output) if self.model.roberta.pooler is not None else None\n",
    "            logits = self.model.classifier(sequence_output)\n",
    "\n",
    "        elif hasattr(self.model, \"bert\"):\n",
    "            extended_attention_mask = self.model.get_extended_attention_mask(\n",
    "                attention_mask, embeddings.shape[:2], embeddings.device\n",
    "            )\n",
    "\n",
    "            encoder_outputs = self.model.bert.encoder(\n",
    "                embeddings,\n",
    "                attention_mask=extended_attention_mask,\n",
    "                head_mask=head_mask,\n",
    "            )\n",
    "            sequence_output = encoder_outputs[0]\n",
    "            pooled_output = self.model.bert.pooler(sequence_output) if self.model.bert.pooler is not None else None\n",
    "            pooled_output = self.model.dropout(pooled_output)\n",
    "            logits = self.model.classifier(pooled_output)\n",
    "        else:\n",
    "            raise ValueError(\"Model not supported\")\n",
    "\n",
    "        return logits\n",
    "    \n",
    "model_wrapper = BertEmbeddingModelWrapper(model)\n",
    "model_wrapper.to(device)\n",
    "model_wrapper.eval()\n",
    "embeddings = get_embeddings(model, input_ids, attention_mask, token_type_ids, position_ids)\n",
    "embeddings.requires_grad_(True)\n",
    "explainer = InputXGradient(model_wrapper)\n",
    "\n",
    "attribution = explainer.attribute(\n",
    "                    inputs=(embeddings),\n",
    "                    target=[1],\n",
    "                    additional_forward_args=(attention_mask,),\n",
    "                )\n",
    "attr = torch.norm(attribution, dim=-1)\n",
    "print(attr)\n",
    "attr_loss = attr.sum()\n",
    "print(attr_loss)\n",
    "model_wrapper.zero_grad()\n",
    "model.zero_grad()\n",
    "attr_loss.backward()\n",
    "\n",
    "for name, param in model_wrapper.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"Gradient for {name}:\")\n",
    "        print(param.grad)\n",
    "    else:\n",
    "        print(f\"No gradient computed for {name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bcos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
