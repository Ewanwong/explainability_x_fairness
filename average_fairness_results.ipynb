{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "724b93c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from utils.vocabulary import *\n",
    "\n",
    "data = \"civil\"\n",
    "root_dir = f\"/scratch/yifwang/fairness_x_explainability/encoder_results_{data}\"\n",
    "\n",
    "\n",
    "models = [\"bert\"] # [\"bert\", \"roberta\", \"distilbert\"]\n",
    "bias_types = [\"race\", \"gender\", \"religion\"]  # [\"race\", \"gender\", \"religion\"]\n",
    "\n",
    "debiasing_methods = [\"no_debiasing\", \"group_balance\", \"group_class_balance\", \"cda\", \"dropout\", \"attention_entropy\", \"causal_debias\"]\n",
    "\n",
    "training_types = [\"all axes\", \"one axis\"]\n",
    "if data == \"civil\":\n",
    "    num_examples = {\"race\": 2000, \"gender\": 2000, \"religion\": 1000}\n",
    "elif data == \"jigsaw\":\n",
    "    num_examples = {\"race\": 400, \"gender\": 800, \"religion\": 200}\n",
    "    \n",
    "fairness_metrics = [\"model_accuracy\", \"accuracy\", \"fpr\", \"fnr\", \"individual_fairness\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97d80d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_dict = {\"model\": [], \"bias_type\": [], \"debiasing_method\": [], \"training_data\": [], \"fairness_metric\": [], \"score\": []}\n",
    "for model in models:\n",
    "    for bias_type in bias_types:\n",
    "        groups = SOCIAL_GROUPS[bias_type]\n",
    "        for training_type in training_types:\n",
    "            data_token = \"all\" if training_type == \"all axes\" else bias_type\n",
    "            for debiasing_method in debiasing_methods:\n",
    "                \n",
    "                file_path = os.path.join(root_dir, f\"{model}_{data}_{data_token}_{bias_type}_test_{num_examples[bias_type]}\", debiasing_method, \"fairness\", f\"fairness_{bias_type}_test_summary_stats.json\")\n",
    "                if not os.path.exists(file_path):\n",
    "                    print(f\"File not found: {file_path}\")\n",
    "                    continue\n",
    "                with open(file_path, \"r\") as f:\n",
    "                    fairness_data = json.load(f)\n",
    "                \n",
    "                for metric in fairness_metrics:\n",
    "                    fairness_dict['model'].append(model)\n",
    "                    fairness_dict['bias_type'].append(bias_type)\n",
    "                    fairness_dict['debiasing_method'].append(debiasing_method)\n",
    "                    fairness_dict['training_data'].append(training_type)\n",
    "                    if metric != \"individual_fairness\" and metric != \"model_accuracy\":\n",
    "                        fairness_dict['fairness_metric'].append(metric)\n",
    "                        fairness_dict['score'].append(round(sum([abs(fairness_data['Group_Fairness'][\"average\"][group][metric]) for group in groups])*100, 2))\n",
    "                    elif metric == \"individual_fairness\":\n",
    "                        fairness_dict['fairness_metric'].append(\"individual_fairness\")\n",
    "                        fairness_dict['score'].append(round(fairness_data['Individual_Fairness']['overall'][\"predicted_class\"][\"abs_average\"]*100, 2))\n",
    "                    elif metric == \"model_accuracy\":\n",
    "                        fairness_dict['fairness_metric'].append(\"model_accuracy\")\n",
    "                        fairness_dict['score'].append(round(fairness_data[\"Metrics\"][\"overall\"][\"accuracy\"] * 100, 2))\n",
    "                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c253f372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>bias_type</th>\n",
       "      <th>debiasing_method</th>\n",
       "      <th>training_data</th>\n",
       "      <th>fairness_metric</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert</td>\n",
       "      <td>race</td>\n",
       "      <td>no_debiasing</td>\n",
       "      <td>all axes</td>\n",
       "      <td>model_accuracy</td>\n",
       "      <td>78.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert</td>\n",
       "      <td>race</td>\n",
       "      <td>no_debiasing</td>\n",
       "      <td>all axes</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bert</td>\n",
       "      <td>race</td>\n",
       "      <td>no_debiasing</td>\n",
       "      <td>all axes</td>\n",
       "      <td>fpr</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bert</td>\n",
       "      <td>race</td>\n",
       "      <td>no_debiasing</td>\n",
       "      <td>all axes</td>\n",
       "      <td>fnr</td>\n",
       "      <td>8.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bert</td>\n",
       "      <td>race</td>\n",
       "      <td>no_debiasing</td>\n",
       "      <td>all axes</td>\n",
       "      <td>individual_fairness</td>\n",
       "      <td>3.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>bert</td>\n",
       "      <td>religion</td>\n",
       "      <td>causal_debias</td>\n",
       "      <td>one axis</td>\n",
       "      <td>model_accuracy</td>\n",
       "      <td>86.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>bert</td>\n",
       "      <td>religion</td>\n",
       "      <td>causal_debias</td>\n",
       "      <td>one axis</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>16.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>bert</td>\n",
       "      <td>religion</td>\n",
       "      <td>causal_debias</td>\n",
       "      <td>one axis</td>\n",
       "      <td>fpr</td>\n",
       "      <td>8.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>bert</td>\n",
       "      <td>religion</td>\n",
       "      <td>causal_debias</td>\n",
       "      <td>one axis</td>\n",
       "      <td>fnr</td>\n",
       "      <td>30.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>bert</td>\n",
       "      <td>religion</td>\n",
       "      <td>causal_debias</td>\n",
       "      <td>one axis</td>\n",
       "      <td>individual_fairness</td>\n",
       "      <td>2.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    model bias_type debiasing_method training_data      fairness_metric  score\n",
       "0    bert      race     no_debiasing      all axes       model_accuracy  78.30\n",
       "1    bert      race     no_debiasing      all axes             accuracy   2.00\n",
       "2    bert      race     no_debiasing      all axes                  fpr   0.02\n",
       "3    bert      race     no_debiasing      all axes                  fnr   8.44\n",
       "4    bert      race     no_debiasing      all axes  individual_fairness   3.99\n",
       "..    ...       ...              ...           ...                  ...    ...\n",
       "205  bert  religion    causal_debias      one axis       model_accuracy  86.40\n",
       "206  bert  religion    causal_debias      one axis             accuracy  16.40\n",
       "207  bert  religion    causal_debias      one axis                  fpr   8.82\n",
       "208  bert  religion    causal_debias      one axis                  fnr  30.46\n",
       "209  bert  religion    causal_debias      one axis  individual_fairness   2.10\n",
       "\n",
       "[210 rows x 6 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to a pandas DataFrame\n",
    "import pandas as pd\n",
    "fairness_df = pd.DataFrame(fairness_dict)\n",
    "fairness_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca24efbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average fairness score for training type all axes, race:\n",
      "fairness_metric      accuracy    fnr   fpr  individual_fairness  \\\n",
      "debiasing_method                                                  \n",
      "no_debiasing             2.00   8.44  0.02                 3.99   \n",
      "group_balance            3.50   8.83  1.72                 4.13   \n",
      "group_class_balance      1.95   9.33  1.35                 4.83   \n",
      "cda                      2.65  20.35  6.38                 0.60   \n",
      "dropout                  2.45   9.99  0.30                 3.60   \n",
      "attention_entropy        2.10   5.92  1.28                 4.98   \n",
      "causal_debias            2.20  13.13  2.51                 3.54   \n",
      "\n",
      "fairness_metric      model_accuracy  \n",
      "debiasing_method                     \n",
      "no_debiasing                  78.30  \n",
      "group_balance                 79.05  \n",
      "group_class_balance           78.17  \n",
      "cda                           78.08  \n",
      "dropout                       78.08  \n",
      "attention_entropy             78.35  \n",
      "causal_debias                 79.40  \n",
      "\n",
      "\n",
      "Average fairness score for training type all axes, gender:\n",
      "fairness_metric      accuracy    fnr   fpr  individual_fairness  \\\n",
      "debiasing_method                                                  \n",
      "no_debiasing              3.2   8.58  1.11                 0.96   \n",
      "group_balance             2.8  11.08  0.31                 1.17   \n",
      "group_class_balance       1.7   4.66  0.51                 0.93   \n",
      "cda                       2.7   6.92  1.05                 0.46   \n",
      "dropout                   3.1   8.39  1.05                 0.89   \n",
      "attention_entropy         2.3   8.01  0.10                 0.96   \n",
      "causal_debias             2.6   7.44  0.70                 0.80   \n",
      "\n",
      "fairness_metric      model_accuracy  \n",
      "debiasing_method                     \n",
      "no_debiasing                  88.20  \n",
      "group_balance                 88.85  \n",
      "group_class_balance           88.25  \n",
      "cda                           87.70  \n",
      "dropout                       87.60  \n",
      "attention_entropy             87.90  \n",
      "causal_debias                 88.75  \n",
      "\n",
      "\n",
      "Average fairness score for training type all axes, religion:\n",
      "fairness_metric      accuracy    fnr   fpr  individual_fairness  \\\n",
      "debiasing_method                                                  \n",
      "no_debiasing            13.47  23.53  6.24                 1.76   \n",
      "group_balance           13.67  23.91  6.92                 2.15   \n",
      "group_class_balance     14.60  33.13  8.52                 1.37   \n",
      "cda                     14.33  30.23  4.70                 0.71   \n",
      "dropout                 13.47  33.12  5.53                 1.59   \n",
      "attention_entropy       11.67  36.15  6.55                 2.10   \n",
      "causal_debias           12.60  31.28  6.70                 2.12   \n",
      "\n",
      "fairness_metric      model_accuracy  \n",
      "debiasing_method                     \n",
      "no_debiasing                  87.43  \n",
      "group_balance                 87.47  \n",
      "group_class_balance           86.90  \n",
      "cda                           86.83  \n",
      "dropout                       87.67  \n",
      "attention_entropy             87.77  \n",
      "causal_debias                 87.70  \n",
      "\n",
      "\n",
      "Average fairness score for training type one axis, race:\n",
      "fairness_metric      accuracy    fnr   fpr  individual_fairness  \\\n",
      "debiasing_method                                                  \n",
      "no_debiasing             2.05  10.04  0.50                 3.17   \n",
      "group_balance            3.10  10.46  0.25                 3.79   \n",
      "group_class_balance      1.80  10.63  2.42                 4.43   \n",
      "cda                      2.35  18.45  5.88                 0.50   \n",
      "dropout                  2.25  10.82  0.78                 3.43   \n",
      "attention_entropy        2.60  11.71  0.99                 2.95   \n",
      "causal_debias            0.00   7.98  3.90                 3.83   \n",
      "\n",
      "fairness_metric      model_accuracy  \n",
      "debiasing_method                     \n",
      "no_debiasing                  78.38  \n",
      "group_balance                 79.25  \n",
      "group_class_balance           78.00  \n",
      "cda                           76.83  \n",
      "dropout                       78.53  \n",
      "attention_entropy             79.15  \n",
      "causal_debias                 78.80  \n",
      "\n",
      "\n",
      "Average fairness score for training type one axis, gender:\n",
      "fairness_metric      accuracy    fnr   fpr  individual_fairness  \\\n",
      "debiasing_method                                                  \n",
      "no_debiasing             3.30  11.98  0.03                 0.66   \n",
      "group_balance            2.80   5.38  1.73                 0.42   \n",
      "group_class_balance      2.75   7.26  0.99                 0.98   \n",
      "cda                      3.60   7.57  2.00                 0.50   \n",
      "dropout                  2.10   3.50  1.46                 0.52   \n",
      "attention_entropy        2.05   7.11  0.10                 0.67   \n",
      "causal_debias            2.65  10.67  0.46                 0.48   \n",
      "\n",
      "fairness_metric      model_accuracy  \n",
      "debiasing_method                     \n",
      "no_debiasing                  88.05  \n",
      "group_balance                 87.25  \n",
      "group_class_balance           87.02  \n",
      "cda                           86.70  \n",
      "dropout                       88.20  \n",
      "attention_entropy             87.67  \n",
      "causal_debias                 86.17  \n",
      "\n",
      "\n",
      "Average fairness score for training type one axis, religion:\n",
      "fairness_metric      accuracy    fnr    fpr  individual_fairness  \\\n",
      "debiasing_method                                                   \n",
      "no_debiasing            18.07  30.90   5.77                 1.27   \n",
      "group_balance           13.53  30.31  11.53                 2.01   \n",
      "group_class_balance     14.73  33.14   3.09                 0.71   \n",
      "cda                     14.13  24.12   5.67                 0.90   \n",
      "dropout                 15.67  27.16   5.93                 1.51   \n",
      "attention_entropy       15.07  26.52   4.99                 1.58   \n",
      "causal_debias           16.40  30.46   8.82                 2.10   \n",
      "\n",
      "fairness_metric      model_accuracy  \n",
      "debiasing_method                     \n",
      "no_debiasing                  85.93  \n",
      "group_balance                 86.83  \n",
      "group_class_balance           85.77  \n",
      "cda                           84.83  \n",
      "dropout                       85.03  \n",
      "attention_entropy             84.93  \n",
      "causal_debias                 86.40  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 用于显示所有结果\n",
    "# show for each debiasing method, what is there average fairness score (across model types and bias types) for each metric and training type, show in one table, where each row is a debiasing method, and each column is a metric and training type\n",
    "\n",
    "for train_type in training_types:\n",
    "    for bias_type in bias_types:\n",
    "        print(f\"Average fairness score for training type {train_type}, {bias_type}:\")\n",
    "        # show the difference in the average fairness score for each debiasing method compared to no debiasing\n",
    "        avg_score = fairness_df[(fairness_df['training_data'] == train_type) & (fairness_df['bias_type'] == bias_type)].groupby(['debiasing_method', 'fairness_metric'])['score'].mean().unstack().reset_index()\n",
    "        avg_score = avg_score.set_index('debiasing_method')\n",
    "        avg_score = avg_score.reindex(debiasing_methods)\n",
    "        print(avg_score)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7a4824bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.80/86.17/86.40 & \\textcolor{forestgreen}{0.00}/\\textcolor{forestgreen}{2.65}/\\textcolor{forestgreen}{16.40} & \\textcolor{red}{3.90}/\\textcolor{red}{0.46}/\\textcolor{red}{8.82} & \\textcolor{forestgreen}{7.98}/\\textcolor{forestgreen}{10.67}/\\textcolor{forestgreen}{30.46} & \\textcolor{red}{3.83}/\\textcolor{forestgreen}{0.48}/\\textcolor{red}{2.10} \n"
     ]
    }
   ],
   "source": [
    "# 加color coding表示比较\n",
    "# print results as metric1_race/metric1_gender/metric1_religion metric2_race/ ...\n",
    "train_data = \"one axis\"\n",
    "debiasing_method = \"causal_debias\"\n",
    "reported_metrics = [\"model_accuracy\", \"accuracy\", \"fpr\", \"fnr\", \"individual_fairness\"]\n",
    "bias_types_order = [\"race\", \"gender\", \"religion\"]\n",
    "print_string = \"\"\n",
    "for reported_metric in reported_metrics:\n",
    "    for bias_type in bias_types_order:\n",
    "        default_score = fairness_df[(fairness_df['training_data'] == train_data) & (fairness_df['debiasing_method'] == \"no_debiasing\") & (fairness_df['bias_type'] == bias_type) & (fairness_df['fairness_metric'] == reported_metric)]['score'].values[0]\n",
    "        score = fairness_df[(fairness_df['training_data'] == train_data) & (fairness_df['debiasing_method'] == debiasing_method) & (fairness_df['bias_type'] == bias_type) & (fairness_df['fairness_metric'] == reported_metric)]['score'].values[0]\n",
    "        if debiasing_method != \"no_debiasing\" and reported_metric != \"model_accuracy\":\n",
    "            if score < default_score:\n",
    "                print_string += \"\\\\textcolor{forestgreen}{\"\n",
    "                print_string += f\"{score:.2f}\" + \"}/\"\n",
    "            elif score > default_score:\n",
    "                print_string += \"\\\\textcolor{red}{\"\n",
    "                print_string += f\"{score:.2f}\" + \"}/\"\n",
    "            else:\n",
    "                print_string += f\"{score:.2f}\" + \"/\"\n",
    "        else:\n",
    "            print_string += f\"{score:.2f}\" + \"/\"\n",
    "    print_string = print_string[:-1] + \" & \"\n",
    "\n",
    "print(print_string[:-2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f638651d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metric: accuracy\n",
      "\n",
      "Metric: accuracy, Model: bert, Bias Type: race\n",
      "no_debiasing: 0.012500000000000067\n",
      "group_balance: 0.01750000000000007\n",
      "group_class_balance: 0.012499999999999956\n",
      "cda: 0.032500000000000084\n",
      "dropout: 0.030000000000000027\n",
      "attention_entropy: 0.0050000000000000044\n",
      "causal_debias: 0.010000000000000009\n",
      "\n",
      "Metric: accuracy, Model: bert, Bias Type: gender\n",
      "no_debiasing: 0.011249999999999982\n",
      "group_balance: 0.011249999999999982\n",
      "group_class_balance: 0.008749999999999925\n",
      "cda: 0.018750000000000044\n",
      "dropout: 0.01375000000000004\n",
      "attention_entropy: 0.02750000000000008\n",
      "causal_debias: 0.01375000000000004\n",
      "\n",
      "Metric: accuracy, Model: bert, Bias Type: religion\n",
      "no_debiasing: 0.09333333333333327\n",
      "group_balance: 0.09666666666666668\n",
      "group_class_balance: 0.09999999999999987\n",
      "cda: 0.07000000000000006\n",
      "dropout: 0.07000000000000006\n",
      "attention_entropy: 0.08000000000000007\n",
      "causal_debias: 0.08999999999999986\n",
      "\n",
      "Metric: accuracy, Model: roberta, Bias Type: race\n",
      "no_debiasing: 0.0050000000000000044\n",
      "group_balance: 0.01750000000000007\n",
      "group_class_balance: 0.022499999999999964\n",
      "cda: 0.010000000000000009\n",
      "dropout: 0.025000000000000022\n",
      "attention_entropy: 0.0050000000000000044\n",
      "causal_debias: 0.0024999999999999467\n",
      "\n",
      "Metric: accuracy, Model: roberta, Bias Type: gender\n",
      "no_debiasing: 0.01750000000000007\n",
      "group_balance: 0.03249999999999997\n",
      "group_class_balance: 0.018750000000000044\n",
      "cda: 0.006249999999999978\n",
      "dropout: 0.01749999999999996\n",
      "attention_entropy: 0.025000000000000022\n",
      "causal_debias: 0.03499999999999992\n",
      "\n",
      "Metric: accuracy, Model: roberta, Bias Type: religion\n",
      "no_debiasing: 0.06999999999999995\n",
      "group_balance: 0.09333333333333338\n",
      "group_class_balance: 0.10666666666666669\n",
      "cda: 0.07666666666666666\n",
      "dropout: 0.046666666666666745\n",
      "attention_entropy: 0.06999999999999995\n",
      "causal_debias: 0.09999999999999998\n",
      "\n",
      "Metric: accuracy, Model: distilbert, Bias Type: race\n",
      "no_debiasing: 0.020000000000000018\n",
      "group_balance: 0.01750000000000007\n",
      "group_class_balance: 0.0025000000000000577\n",
      "cda: 0.0025000000000000577\n",
      "dropout: 0.020000000000000018\n",
      "attention_entropy: 0.010000000000000009\n",
      "causal_debias: 0.015000000000000013\n",
      "\n",
      "Metric: accuracy, Model: distilbert, Bias Type: gender\n",
      "no_debiasing: 0.032500000000000084\n",
      "group_balance: 0.02124999999999999\n",
      "group_class_balance: 0.011249999999999982\n",
      "cda: 0.026249999999999996\n",
      "dropout: 0.032500000000000084\n",
      "attention_entropy: 0.03249999999999997\n",
      "causal_debias: 0.020000000000000018\n",
      "\n",
      "Metric: accuracy, Model: distilbert, Bias Type: religion\n",
      "no_debiasing: 0.07999999999999996\n",
      "group_balance: 0.09333333333333327\n",
      "group_class_balance: 0.09000000000000008\n",
      "cda: 0.07999999999999996\n",
      "dropout: 0.07999999999999996\n",
      "attention_entropy: 0.07999999999999996\n",
      "causal_debias: 0.07333333333333336\n",
      "\n",
      "Metric: f1\n",
      "\n",
      "Metric: f1, Model: bert, Bias Type: race\n",
      "no_debiasing: 0.05448403617740005\n",
      "group_balance: 0.02936203016399963\n",
      "group_class_balance: 0.014233095794504935\n",
      "cda: 0.04674957797049806\n",
      "dropout: 0.06485929832115789\n",
      "attention_entropy: 0.00311876667929345\n",
      "causal_debias: 0.004310517746617615\n",
      "\n",
      "Metric: f1, Model: bert, Bias Type: gender\n",
      "no_debiasing: 0.027446713885872942\n",
      "group_balance: 0.035489665253548575\n",
      "group_class_balance: 0.044514101420435104\n",
      "cda: 0.04940907792345994\n",
      "dropout: 0.05115727096180711\n",
      "attention_entropy: 0.07355200224969916\n",
      "causal_debias: 0.04286554543017007\n",
      "\n",
      "Metric: f1, Model: bert, Bias Type: religion\n",
      "no_debiasing: 0.15218870251187877\n",
      "group_balance: 0.1707928984532775\n",
      "group_class_balance: 0.2958990536465784\n",
      "cda: 0.11825208100915174\n",
      "dropout: 0.10901398975806409\n",
      "attention_entropy: 0.1873495758848598\n",
      "causal_debias: 0.17116778561862867\n",
      "\n",
      "Metric: f1, Model: roberta, Bias Type: race\n",
      "no_debiasing: 0.015318250898300856\n",
      "group_balance: 0.04372521240237459\n",
      "group_class_balance: 0.05288461460916549\n",
      "cda: 0.05090327665212746\n",
      "dropout: 0.08827804361916314\n",
      "attention_entropy: 0.020884696113296908\n",
      "causal_debias: 0.009536259681794057\n",
      "\n",
      "Metric: f1, Model: roberta, Bias Type: gender\n",
      "no_debiasing: 0.054955173072914865\n",
      "group_balance: 0.09219791227410157\n",
      "group_class_balance: 0.0638489512071917\n",
      "cda: 0.038974471602173155\n",
      "dropout: 0.059203523578569506\n",
      "attention_entropy: 0.07055291146200238\n",
      "causal_debias: 0.10947210742426827\n",
      "\n",
      "Metric: f1, Model: roberta, Bias Type: religion\n",
      "no_debiasing: 0.11956360050457338\n",
      "group_balance: 0.23713185677464654\n",
      "group_class_balance: 0.2978501580779186\n",
      "cda: 0.13086700007247953\n",
      "dropout: 0.05810586194450862\n",
      "attention_entropy: 0.1410427472798571\n",
      "causal_debias: 0.24069828943239135\n",
      "\n",
      "Metric: f1, Model: distilbert, Bias Type: race\n",
      "no_debiasing: 0.04671377119359488\n",
      "group_balance: 0.013692210309011021\n",
      "group_class_balance: 0.0175492265184386\n",
      "cda: 0.02347768099235481\n",
      "dropout: 0.04671377119359488\n",
      "attention_entropy: 0.02734777688135903\n",
      "causal_debias: 0.020804341963582607\n",
      "\n",
      "Metric: f1, Model: distilbert, Bias Type: gender\n",
      "no_debiasing: 0.11627677357905652\n",
      "group_balance: 0.07537577926713401\n",
      "group_class_balance: 0.048862249111034206\n",
      "cda: 0.09088649215665068\n",
      "dropout: 0.11627677357905652\n",
      "attention_entropy: 0.11387213723993805\n",
      "causal_debias: 0.07248883304146791\n",
      "\n",
      "Metric: f1, Model: distilbert, Bias Type: religion\n",
      "no_debiasing: 0.17669007688549843\n",
      "group_balance: 0.19968329541846386\n",
      "group_class_balance: 0.28947457095871776\n",
      "cda: 0.17757493733609686\n",
      "dropout: 0.17669007688549843\n",
      "attention_entropy: 0.15761691823688184\n",
      "causal_debias: 0.1297976852609385\n",
      "\n",
      "Metric: fpr\n",
      "\n",
      "Metric: fpr, Model: bert, Bias Type: race\n",
      "no_debiasing: 0.0158726989577926\n",
      "group_balance: 0.015623128219462518\n",
      "group_class_balance: 0.01266821067763447\n",
      "cda: 0.02858084095355988\n",
      "dropout: 0.012558399552769235\n",
      "attention_entropy: 0.006508804855648288\n",
      "causal_debias: 0.02221179571137643\n",
      "\n",
      "Metric: fpr, Model: bert, Bias Type: gender\n",
      "no_debiasing: 0.01514375776100869\n",
      "group_balance: 0.010959977075174324\n",
      "group_class_balance: 0.004074887763874296\n",
      "cda: 0.017810679147960647\n",
      "dropout: 0.011000095520106983\n",
      "attention_entropy: 0.02615531569395358\n",
      "causal_debias: 0.013752985003343205\n",
      "\n",
      "Metric: fpr, Model: bert, Bias Type: religion\n",
      "no_debiasing: 0.04647138796193301\n",
      "group_balance: 0.04660516041789352\n",
      "group_class_balance: 0.020920848873475978\n",
      "cda: 0.0402349742269915\n",
      "dropout: 0.05598813496477568\n",
      "attention_entropy: 0.05190952910641454\n",
      "causal_debias: 0.04270685656539219\n",
      "\n",
      "Metric: fpr, Model: roberta, Bias Type: race\n",
      "no_debiasing: 0.00011979395439843527\n",
      "group_balance: 0.00014974244299804929\n",
      "group_class_balance: 0.006239268458251807\n",
      "cda: 0.015862716128259395\n",
      "dropout: 0.012678193507167673\n",
      "attention_entropy: 0.009623447670007584\n",
      "causal_debias: 0.00010981112486523059\n",
      "\n",
      "Metric: fpr, Model: roberta, Bias Type: gender\n",
      "no_debiasing: 0.015120832935332887\n",
      "group_balance: 0.024747349317031233\n",
      "group_class_balance: 0.013712866558410548\n",
      "cda: 0.0012933422485433173\n",
      "dropout: 0.015138026554589741\n",
      "attention_entropy: 0.020603687076129525\n",
      "causal_debias: 0.02616677810679148\n",
      "\n",
      "Metric: fpr, Model: roberta, Bias Type: religion\n",
      "no_debiasing: 0.05556064472507579\n",
      "group_balance: 0.041156840934371525\n",
      "group_class_balance: 0.025831170436287236\n",
      "cda: 0.017988033181385275\n",
      "dropout: 0.04193184874988186\n",
      "attention_entropy: 0.03161682915657921\n",
      "causal_debias: 0.05413422321097515\n",
      "\n",
      "Metric: fpr, Model: distilbert, Bias Type: race\n",
      "no_debiasing: 0.0062292856287186026\n",
      "group_balance: 0.02537635267340175\n",
      "group_class_balance: 0.019047238749351117\n",
      "cda: 0.009533602204208761\n",
      "dropout: 0.0062292856287186026\n",
      "attention_entropy: 8.984546579882471e-05\n",
      "causal_debias: 0.012758056143433297\n",
      "\n",
      "Metric: fpr, Model: distilbert, Bias Type: gender\n",
      "no_debiasing: 0.017879453624988058\n",
      "group_balance: 0.012345018626420862\n",
      "group_class_balance: 0.005425542076607125\n",
      "cda: 0.01927022638265355\n",
      "dropout: 0.017879453624988058\n",
      "attention_entropy: 0.01786799121215016\n",
      "causal_debias: 0.00955201069825198\n",
      "\n",
      "Metric: fpr, Model: distilbert, Bias Type: religion\n",
      "no_debiasing: 0.013976313551004385\n",
      "group_balance: 0.03732542330985045\n",
      "group_class_balance: 0.025223378190727532\n",
      "cda: 0.016504903778344855\n",
      "dropout: 0.013976313551004385\n",
      "attention_entropy: 0.01780773117552546\n",
      "causal_debias: 0.04712280166052331\n",
      "\n",
      "Metric: fnr\n",
      "\n",
      "Metric: fnr, Model: bert, Bias Type: race\n",
      "no_debiasing: 0.12693631669535288\n",
      "group_balance: 0.030981067125645523\n",
      "group_class_balance: 0.003729202524383246\n",
      "cda: 0.041738382099827886\n",
      "dropout: 0.10240963855421692\n",
      "attention_entropy: 0.005737234652897327\n",
      "causal_debias: 0.04345955249569711\n",
      "\n",
      "Metric: fnr, Model: bert, Bias Type: gender\n",
      "no_debiasing: 0.0035897435897435104\n",
      "group_balance: 0.03230769230769237\n",
      "group_class_balance: 0.07487179487179485\n",
      "cda: 0.04410256410256408\n",
      "dropout: 0.06102564102564101\n",
      "attention_entropy: 0.055384615384615365\n",
      "causal_debias: 0.03384615384615386\n",
      "\n",
      "Metric: fnr, Model: bert, Bias Type: religion\n",
      "no_debiasing: 0.2175824175824177\n",
      "group_balance: 0.2678876678876679\n",
      "group_class_balance: 0.5807081807081809\n",
      "cda: 0.38241758241758245\n",
      "dropout: 0.1316239316239316\n",
      "attention_entropy: 0.3484737484737485\n",
      "causal_debias: 0.24102564102564106\n",
      "\n",
      "Metric: fnr, Model: roberta, Bias Type: race\n",
      "no_debiasing: 0.030550774526678204\n",
      "group_balance: 0.09007458405048774\n",
      "group_class_balance: 0.09050487664945506\n",
      "cda: 0.11531841652323582\n",
      "dropout: 0.17512908777969016\n",
      "attention_entropy: 0.06540447504302926\n",
      "causal_debias: 0.018789443488238633\n",
      "\n",
      "Metric: fnr, Model: roberta, Bias Type: gender\n",
      "no_debiasing: 0.05743589743589744\n",
      "group_balance: 0.11897435897435898\n",
      "group_class_balance: 0.08615384615384619\n",
      "cda: 0.07487179487179485\n",
      "dropout: 0.062051282051281964\n",
      "attention_entropy: 0.08051282051282055\n",
      "causal_debias: 0.13692307692307698\n",
      "\n",
      "Metric: fnr, Model: roberta, Bias Type: religion\n",
      "no_debiasing: 0.31135531135531136\n",
      "group_balance: 0.4029304029304029\n",
      "group_class_balance: 0.6434676434676436\n",
      "cda: 0.3128205128205128\n",
      "dropout: 0.28717948717948705\n",
      "attention_entropy: 0.24590964590964598\n",
      "causal_debias: 0.34139194139194146\n",
      "\n",
      "Metric: fnr, Model: distilbert, Bias Type: race\n",
      "no_debiasing: 0.07860011474469308\n",
      "group_balance: 0.018932874354561147\n",
      "group_class_balance: 0.06683878370625351\n",
      "cda: 0.055651176133103775\n",
      "dropout: 0.07860011474469308\n",
      "attention_entropy: 0.05436029833620204\n",
      "causal_debias: 0.017498565691336787\n",
      "\n",
      "Metric: fnr, Model: distilbert, Bias Type: gender\n",
      "no_debiasing: 0.18871794871794872\n",
      "group_balance: 0.12205128205128207\n",
      "group_class_balance: 0.08564102564102571\n",
      "cda: 0.11435897435897435\n",
      "dropout: 0.18871794871794872\n",
      "attention_entropy: 0.18820512820512825\n",
      "causal_debias: 0.13435897435897437\n",
      "\n",
      "Metric: fnr, Model: distilbert, Bias Type: religion\n",
      "no_debiasing: 0.43614163614163615\n",
      "group_balance: 0.4810744810744811\n",
      "group_class_balance: 0.49377289377289374\n",
      "cda: 0.36410256410256414\n",
      "dropout: 0.43614163614163615\n",
      "attention_entropy: 0.4105006105006105\n",
      "causal_debias: 0.19438339438339441\n",
      "\n",
      "Metric: individual_fairness\n",
      "\n",
      "Metric: individual_fairness, Model: bert, Bias Type: race\n",
      "no_debiasing: 0.012983148917555809\n",
      "group_balance: 0.028113659471273422\n",
      "group_class_balance: 0.012917093932628632\n",
      "cda: 0.006868772208690643\n",
      "dropout: 0.019112132489681244\n",
      "attention_entropy: 0.025679398328065872\n",
      "causal_debias: 0.014046970754861832\n",
      "\n",
      "Metric: individual_fairness, Model: bert, Bias Type: gender\n",
      "no_debiasing: 0.003321267431601882\n",
      "group_balance: 0.003955107182264328\n",
      "group_class_balance: 0.002756318775936961\n",
      "cda: 0.002886065049096942\n",
      "dropout: 0.0033371844328939915\n",
      "attention_entropy: 0.004069846123456955\n",
      "causal_debias: 0.00404489878565073\n",
      "\n",
      "Metric: individual_fairness, Model: bert, Bias Type: religion\n",
      "no_debiasing: 0.011753994040191174\n",
      "group_balance: 0.007588961627334356\n",
      "group_class_balance: 0.004664476495236158\n",
      "cda: 0.004594665020704269\n",
      "dropout: 0.012739547528326511\n",
      "attention_entropy: 0.010666598565876484\n",
      "causal_debias: 0.010024911724030972\n",
      "\n",
      "Metric: individual_fairness, Model: roberta, Bias Type: race\n",
      "no_debiasing: 0.025236841291189194\n",
      "group_balance: 0.027636919170618057\n",
      "group_class_balance: 0.04753020778298378\n",
      "cda: 0.00518218707293272\n",
      "dropout: 0.010244287550449371\n",
      "attention_entropy: 0.0339779406785965\n",
      "causal_debias: 0.025470055639743805\n",
      "\n",
      "Metric: individual_fairness, Model: roberta, Bias Type: gender\n",
      "no_debiasing: 0.0035228095948696136\n",
      "group_balance: 0.0029784217476844788\n",
      "group_class_balance: 0.002329776529222727\n",
      "cda: 0.0023390627466142178\n",
      "dropout: 0.003257093485444784\n",
      "attention_entropy: 0.0037621469236910343\n",
      "causal_debias: 0.004008290357887745\n",
      "\n",
      "Metric: individual_fairness, Model: roberta, Bias Type: religion\n",
      "no_debiasing: 0.015500213019549847\n",
      "group_balance: 0.009626544080674648\n",
      "group_class_balance: 0.003412158926948905\n",
      "cda: 0.00738936522975564\n",
      "dropout: 0.007926114834845066\n",
      "attention_entropy: 0.01187202800065279\n",
      "causal_debias: 0.008038538508117199\n",
      "\n",
      "Metric: individual_fairness, Model: distilbert, Bias Type: race\n",
      "no_debiasing: 0.01694110780954361\n",
      "group_balance: 0.020641708746552467\n",
      "group_class_balance: 0.021096346899867058\n",
      "cda: 0.003738864092156291\n",
      "dropout: 0.01694110780954361\n",
      "attention_entropy: 0.019453339278697968\n",
      "causal_debias: 0.0190929863601923\n",
      "\n",
      "Metric: individual_fairness, Model: distilbert, Bias Type: gender\n",
      "no_debiasing: 0.005867723375558853\n",
      "group_balance: 0.0033747637644410133\n",
      "group_class_balance: 0.0038103556726127863\n",
      "cda: 0.002073997398838401\n",
      "dropout: 0.005867723375558853\n",
      "attention_entropy: 0.005721203982830048\n",
      "causal_debias: 0.00575836468487978\n",
      "\n",
      "Metric: individual_fairness, Model: distilbert, Bias Type: religion\n",
      "no_debiasing: 0.010348327457904816\n",
      "group_balance: 0.00915437564253807\n",
      "group_class_balance: 0.006703247781842947\n",
      "cda: 0.0028312390204519033\n",
      "dropout: 0.010348327457904816\n",
      "attention_entropy: 0.010470685549080372\n",
      "causal_debias: 0.009706749580800533\n"
     ]
    }
   ],
   "source": [
    "# print out each metric for different debiasing methods, for all models and bias types\n",
    "for metric in fairness_metrics:\n",
    "    print(f\"\\nMetric: {metric}\")\n",
    "    for model in models:\n",
    "        for bias_type in bias_types:\n",
    "            print(f\"\\nMetric: {metric}, Model: {model}, Bias Type: {bias_type}\")\n",
    "            for debiasing_method in debiasing_methods:\n",
    "                subset = fairness_df[(fairness_df['model'] == model) & (fairness_df['bias_type'] == bias_type) & (fairness_df['debiasing_method'] == debiasing_method) & (fairness_df['fairness_metric'] == metric)]\n",
    "                if not subset.empty:\n",
    "                    print(f\"{debiasing_method}: {subset['score'].values[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20095605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average fairness score for accuracy:\n",
      "  bias_type     score\n",
      "0    gender  0.020357\n",
      "1      race  0.014405\n",
      "2  religion  0.081905\n",
      "Average fairness score for f1:\n",
      "  bias_type     score\n",
      "0    gender  0.066184\n",
      "1      race  0.029809\n",
      "2  religion  0.170914\n",
      "Average fairness score for fpr:\n",
      "  bias_type     score\n",
      "0    gender  0.015972\n",
      "1      race  0.014296\n",
      "2  religion  0.032984\n",
      "Average fairness score for fnr:\n",
      "  bias_type     score\n",
      "0    gender  0.086337\n",
      "1      race  0.058762\n",
      "2  religion  0.332525\n",
      "Average fairness score for individual_fairness:\n",
      "  bias_type     score\n",
      "0    gender  0.003858\n",
      "1      race  0.019275\n",
      "2  religion  0.008408\n"
     ]
    }
   ],
   "source": [
    "# show the average fairness score for each bias type\n",
    "for metric in fairness_metrics:\n",
    "    print(f\"Average fairness score for {metric}:\")\n",
    "    avg_score = fairness_df[fairness_df['fairness_metric'] == metric].groupby(['bias_type'])['score'].mean().reset_index()\n",
    "    print(avg_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4119dbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average fairness score for training type all axes:\n",
      "fairness_metric      accuracy        f1       fnr       fpr  \\\n",
      "debiasing_method                                              \n",
      "no_debiasing         0.038009  0.084849  0.161212  0.020708   \n",
      "group_balance        0.044537  0.099717  0.173913  0.023810   \n",
      "group_class_balance  0.041435  0.125013  0.236188  0.014794   \n",
      "cda                  0.035880  0.080788  0.167265  0.018564   \n",
      "dropout              0.037269  0.085589  0.169209  0.020820   \n",
      "attention_entropy    0.037222  0.088371  0.161610  0.020243   \n",
      "causal_debias        0.039954  0.089016  0.129075  0.025391   \n",
      "\n",
      "fairness_metric      individual_fairness  \n",
      "debiasing_method                          \n",
      "no_debiasing                    0.011719  \n",
      "group_balance                   0.012563  \n",
      "group_class_balance             0.011691  \n",
      "cda                             0.004212  \n",
      "dropout                         0.009975  \n",
      "attention_entropy               0.013964  \n",
      "causal_debias                   0.011132  \n",
      "\n",
      "\n",
      "Average fairness score for training type one axis:\n",
      "fairness_metric      accuracy        f1       fnr       fpr  \\\n",
      "debiasing_method                                              \n",
      "no_debiasing         0.037222  0.072334  0.136093  0.026196   \n",
      "group_balance        0.042546  0.086854  0.147057  0.015164   \n",
      "group_class_balance  0.025093  0.072019  0.170557  0.010985   \n",
      "cda                  0.048565  0.111161  0.170381  0.035540   \n",
      "dropout              0.038056  0.079905  0.122421  0.023473   \n",
      "attention_entropy    0.033380  0.068532  0.115268  0.020847   \n",
      "causal_debias        0.045278  0.101418  0.168667  0.018639   \n",
      "\n",
      "fairness_metric      individual_fairness  \n",
      "debiasing_method                          \n",
      "no_debiasing                    0.011205  \n",
      "group_balance                   0.012053  \n",
      "group_class_balance             0.008409  \n",
      "cda                             0.003552  \n",
      "dropout                         0.011742  \n",
      "attention_entropy               0.011611  \n",
      "causal_debias                   0.013366  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show for each debiasing method, what is there average fairness score (across model types and bias types) for each metric and training type, show in one table, where each row is a debiasing method, and each column is a metric and training type\n",
    "\n",
    "for train_type in training_types:\n",
    "    print(f\"Average fairness score for training type {train_type}:\")\n",
    "    # show the difference in the average fairness score for each debiasing method compared to no debiasing\n",
    "    avg_score = fairness_df[fairness_df['training_data'] == train_type].groupby(['debiasing_method', 'fairness_metric'])['score'].mean().unstack().reset_index()\n",
    "    avg_score = avg_score.set_index('debiasing_method')\n",
    "    avg_score = avg_score.reindex(debiasing_methods)\n",
    "    print(avg_score)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83c42919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average fairness score for training type all axes:\n",
      "fairness_metric      accuracy        f1       fnr       fpr  \\\n",
      "debiasing_method                                              \n",
      "no_debiasing         0.000000  0.000000  0.000000  0.000000   \n",
      "group_balance        0.006528  0.014868  0.012700  0.003102   \n",
      "group_class_balance  0.003426  0.040164  0.074975 -0.005915   \n",
      "cda                 -0.002130 -0.004060  0.006052 -0.002144   \n",
      "dropout             -0.000741  0.000740  0.007997  0.000112   \n",
      "attention_entropy   -0.000787  0.003522  0.000398 -0.000466   \n",
      "causal_debias        0.001944  0.004167 -0.032137  0.004682   \n",
      "\n",
      "fairness_metric      individual_fairness  \n",
      "debiasing_method                          \n",
      "no_debiasing                    0.000000  \n",
      "group_balance                   0.000844  \n",
      "group_class_balance            -0.000028  \n",
      "cda                            -0.007508  \n",
      "dropout                        -0.001745  \n",
      "attention_entropy               0.002244  \n",
      "causal_debias                  -0.000587  \n",
      "\n",
      "\n",
      "Average fairness score for training type one axis:\n",
      "fairness_metric      accuracy        f1       fnr       fpr  \\\n",
      "debiasing_method                                              \n",
      "no_debiasing         0.000000  0.000000  0.000000  0.000000   \n",
      "group_balance        0.005324  0.014520  0.010964 -0.011032   \n",
      "group_class_balance -0.012130 -0.000315  0.034464 -0.015211   \n",
      "cda                  0.011343  0.038827  0.034288  0.009344   \n",
      "dropout              0.000833  0.007571 -0.013672 -0.002723   \n",
      "attention_entropy   -0.003843 -0.003802 -0.020825 -0.005349   \n",
      "causal_debias        0.008056  0.029084  0.032574 -0.007557   \n",
      "\n",
      "fairness_metric      individual_fairness  \n",
      "debiasing_method                          \n",
      "no_debiasing                    0.000000  \n",
      "group_balance                   0.000848  \n",
      "group_class_balance            -0.002795  \n",
      "cda                            -0.007653  \n",
      "dropout                         0.000538  \n",
      "attention_entropy               0.000406  \n",
      "causal_debias                   0.002161  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show for each debiasing method, what is there average fairness score (across model types and bias types) for each metric and training type, show in one table, where each row is a debiasing method, and each column is a metric and training type\n",
    "\n",
    "for train_type in training_types:\n",
    "    print(f\"Average fairness score for training type {train_type}:\")\n",
    "    # show the difference in the average fairness score for each debiasing method compared to no debiasing\n",
    "    avg_score = fairness_df[fairness_df['training_data'] == train_type].groupby(['debiasing_method', 'fairness_metric'])['score'].mean().unstack().reset_index()\n",
    "    avg_score = avg_score.set_index('debiasing_method')\n",
    "    avg_score = avg_score.reindex(debiasing_methods)\n",
    "    # for each debiasing method and metric, calculate the difference from no debiasing of the same metric\n",
    "    no_debiasing_scores = avg_score.loc['no_debiasing']\n",
    "    for metric in avg_score.columns:\n",
    "        if metric != 'debiasing_method':\n",
    "            avg_score[metric] = avg_score[metric] - no_debiasing_scores[metric]\n",
    "    print(avg_score)\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bcos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
