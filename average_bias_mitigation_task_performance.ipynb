{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814d2ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from utils.vocabulary import *\n",
    "\n",
    "data = \"civil\"  # \"civil\" or \"jigsaw\"\n",
    "root_dir = f\"/scratch/yifwang/new_fairness_x_explainability/encoder_results_{data}\"\n",
    "debiased_dir = f\"/scratch/yifwang/new_fairness_x_explainability/bias_mitigation_results_{data}\"\n",
    "\n",
    "\n",
    "models = [\"bert\"] # [\"bert\", \"roberta\", \"distilbert\"]\n",
    "bias_types = [\"race\"]  # [\"race\", \"gender\", \"religion\"]\n",
    "seeds = [42]\n",
    "\n",
    "debiasing_methods = [\"no_debiasing\", \"group_balance\", \"group_class_balance\", \"cda\", \"dropout\", \"attention_entropy\", \"causal_debias\"]\n",
    "\n",
    "explanation_methods = [\"Saliency\"] # [\"Saliency\", \"InputXGradient\", \"IntegratedGradients\", \"raw_attention\", \"attention_rollout\", \"attention_flow\", \"occlusion\"]\n",
    "aggregation = [\"L1\", \"L2\"]\n",
    "alphas = [1.0, 0.1, 0.01, 0.001, 0.0001]\n",
    "\n",
    "training_types = [\"one axis\"] # [\"all axes\", \"one axis\"]\n",
    "if data == \"civil\":\n",
    "    num_examples = {\"race\": 2000, \"gender\": 2000, \"religion\": 1000}\n",
    "elif data == \"jigsaw\":\n",
    "    num_examples = {\"race\": 400, \"gender\": 800, \"religion\": 200}\n",
    "    \n",
    "fairness_metrics = [\"accuracy\", \"f1\", \"fpr\", \"fnr\", \"individual_fairness\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965ede62",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_dict = {\"model\": [], \"bias_type\": [], \"debiasing_method\": [], \"training_data\": [], \"metrics\": [], \"score\": [], \"seed\": []}\n",
    "for model in models:\n",
    "    for bias_type in bias_types:\n",
    "        groups = SOCIAL_GROUPS[bias_type]\n",
    "        # load baseline fairness results\n",
    "        for training_type in training_types:\n",
    "            data_token = \"all\" if training_type == \"all axes\" else bias_type\n",
    "            for debiasing_method in debiasing_methods:\n",
    "                \n",
    "                file_path = os.path.join(root_dir, f\"{model}_{data}_{data_token}_{bias_type}_test_{num_examples[bias_type]}\", debiasing_method, \"fairness\", f\"fairness_{bias_type}_test_summary_stats.json\")\n",
    "                if not os.path.exists(file_path):\n",
    "                    print(f\"File not found: {file_path}\")\n",
    "                    continue\n",
    "                with open(file_path, \"r\") as f:\n",
    "                    fairness_data = json.load(f)\n",
    "                \n",
    "                \n",
    "                for metric in [\"accuracy\", \"f1\"]:\n",
    "                    fairness_dict['model'].append(model)\n",
    "                    fairness_dict['bias_type'].append(bias_type)\n",
    "                    fairness_dict['debiasing_method'].append(debiasing_method)\n",
    "                    fairness_dict[\"seed\"].append(42)\n",
    "                    fairness_dict['training_data'].append(training_type)\n",
    "                    fairness_dict['metrics'].append(f\"task_{metric}\")\n",
    "                    if metric in fairness_data['Metrics']['overall']:\n",
    "                        fairness_dict['score'].append(fairness_data['Metrics']['overall'][metric])\n",
    "                    else:\n",
    "                        print(f\"Metric {metric} not found in fairness data for {debiasing_method} on {bias_type}\")\n",
    "                        fairness_dict['score'].append(None)\n",
    " \n",
    "\n",
    "            for explanation_method in explanation_methods:\n",
    "                for aggregation_method in aggregation:\n",
    "                    for alpha in alphas:\n",
    "                        for seed in seeds:\n",
    "                            file_path = os.path.join(debiased_dir, f\"{model}_{data}_{data_token}_{bias_type}_{split}_{num_examples[bias_type]}_{seed}\", explanation_method, f\"{aggregation_method}_{alpha}\",\"fairness\", f\"fairness_{bias_type}_{split}_summary_stats.json\")\n",
    "                            if not os.path.exists(file_path):\n",
    "                                print(f\"File not found: {file_path}\")\n",
    "                                continue\n",
    "                            with open(file_path, \"r\") as f:\n",
    "                                explanation_data = json.load(f)\n",
    "                            \n",
    "                            for metric in [\"accuracy\", \"f1\"]:\n",
    "                                fairness_dict['model'].append(model)\n",
    "                                fairness_dict['bias_type'].append(bias_type)\n",
    "                                fairness_dict['debiasing_method'].append(f\"{explanation_method}_{aggregation_method}_{alpha}\")\n",
    "                                fairness_dict['training_data'].append(training_type)\n",
    "                                fairness_dict['metrics'].append(f\"task_{metric}\")\n",
    "                                fairness_dict['seed'].append(seed)\n",
    "                                if metric in explanation_data['Metrics']['overall']:\n",
    "                                    fairness_dict['score'].append(explanation_data['Metrics']['overall'][metric])\n",
    "                                else:\n",
    "                                    print(f\"Metric {metric} not found in explanation data for {explanation_method} on {bias_type}\")\n",
    "                                    fairness_dict['score'].append(None)\n",
    "                            for metric in fairness_metrics:\n",
    "                                fairness_dict['model'].append(model)\n",
    "                                fairness_dict['bias_type'].append(bias_type)\n",
    "                                fairness_dict['debiasing_method'].append(f\"{explanation_method}_{aggregation_method}_{alpha}\")\n",
    "                                fairness_dict['training_data'].append(training_type)\n",
    "                                fairness_dict['seed'].append(seed)\n",
    "                                if metric != \"individual_fairness\":\n",
    "                                    fairness_dict['metric'].append(metric)\n",
    "                                    fairness_dict['score'].append(sum([abs(fairness_data['Group_Fairness'][\"average\"][group][metric]) for group in groups]))\n",
    "                                else:\n",
    "                                    fairness_dict['metric'].append(\"individual_fairness\")\n",
    "                                    fairness_dict['score'].append(fairness_data['Individual_Fairness']['overall'][\"predicted_class\"][\"abs_average\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61002642",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fairness_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# convert to a pandas DataFrame\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m fairness_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mfairness_dict\u001b[49m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# fairness_df\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fairness_dict' is not defined"
     ]
    }
   ],
   "source": [
    "# convert to a pandas DataFrame\n",
    "import pandas as pd\n",
    "fairness_df = pd.DataFrame(fairness_dict)\n",
    "# fairness_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d87784a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average fairness score for training type one axis:\n",
      "metrics              accuracy        f1\n",
      "debiasing_method                       \n",
      "no_debiasing          0.78375  0.751909\n",
      "group_balance         0.79250  0.775724\n",
      "group_class_balance   0.78000  0.762283\n",
      "cda                   0.76825  0.744694\n",
      "dropout               0.78525  0.755417\n",
      "attention_entropy     0.79150  0.765320\n",
      "causal_debias         0.78800  0.772503\n",
      "Saliency_L1_1.0       0.76975  0.756139\n",
      "Saliency_L1_0.1       0.76700  0.747938\n",
      "Saliency_L1_0.01      0.77625  0.750052\n",
      "Saliency_L1_0.001     0.77075  0.753470\n",
      "Saliency_L1_0.0001    0.78425  0.763416\n",
      "Saliency_L2_1.0       0.77950  0.757931\n",
      "Saliency_L2_0.1       0.78275  0.764941\n",
      "Saliency_L2_0.01      0.77800  0.757845\n",
      "Saliency_L2_0.001     0.77100  0.747934\n",
      "Saliency_L2_0.0001    0.77975  0.762604\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show for each debiasing method, what is there average fairness score (across model types and bias types) for each metric and training type, show in one table, where each row is a debiasing method, and each column is a metric and training type\n",
    "\n",
    "for train_type in training_types:\n",
    "    print(f\"Average fairness score for training type {train_type}:\")\n",
    "    # show the difference in the average fairness score for each debiasing method compared to no debiasing\n",
    "    avg_score = fairness_df[fairness_df['training_data'] == train_type].groupby(['debiasing_method', 'metrics'])['score'].mean().unstack().reset_index()\n",
    "    avg_score = avg_score.set_index('debiasing_method')\n",
    "    avg_score = avg_score.reindex(fairness_df['debiasing_method'].unique())\n",
    "    print(avg_score)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4433c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average fairness score for training type one axis:\n",
      "metrics              accuracy        f1\n",
      "debiasing_method                       \n",
      "no_debiasing          0.00000  0.000000\n",
      "group_balance         0.00875  0.023814\n",
      "group_class_balance  -0.00375  0.010374\n",
      "cda                  -0.01550 -0.007215\n",
      "dropout               0.00150  0.003508\n",
      "attention_entropy     0.00775  0.013411\n",
      "causal_debias         0.00425  0.020593\n",
      "Saliency_L1_1.0      -0.01400  0.004230\n",
      "Saliency_L1_0.1      -0.01675 -0.003971\n",
      "Saliency_L1_0.01     -0.00750 -0.001857\n",
      "Saliency_L1_0.001    -0.01300  0.001561\n",
      "Saliency_L1_0.0001    0.00050  0.011507\n",
      "Saliency_L2_1.0      -0.00425  0.006022\n",
      "Saliency_L2_0.1      -0.00100  0.013032\n",
      "Saliency_L2_0.01     -0.00575  0.005936\n",
      "Saliency_L2_0.001    -0.01275 -0.003975\n",
      "Saliency_L2_0.0001   -0.00400  0.010694\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show for each debiasing method, what is there average fairness score (across model types and bias types) for each metric and training type, show in one table, where each row is a debiasing method, and each column is a metric and training type\n",
    "\n",
    "for train_type in training_types:\n",
    "    print(f\"Average fairness score for training type {train_type}:\")\n",
    "    # show the difference in the average fairness score for each debiasing method compared to no debiasing\n",
    "    avg_score = fairness_df[fairness_df['training_data'] == train_type].groupby(['debiasing_method', 'metrics'])['score'].mean().unstack().reset_index()\n",
    "    avg_score = avg_score.set_index('debiasing_method')\n",
    "    avg_score = avg_score.reindex(fairness_df['debiasing_method'].unique())\n",
    "    # for each debiasing method and metric, calculate the difference from no debiasing of the same metric\n",
    "    no_debiasing_scores = avg_score.loc['no_debiasing']\n",
    "    for metric in avg_score.columns:\n",
    "        if metric != 'debiasing_method':\n",
    "            avg_score[metric] = avg_score[metric] - no_debiasing_scores[metric]\n",
    "    print(avg_score)\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bcos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
