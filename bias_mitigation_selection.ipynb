{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdcbc055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from utils.vocabulary import *\n",
    "\n",
    "data = \"civil\"  # \"civil\" or \"jigsaw\"\n",
    "root_dir = f\"/scratch/yifwang/fairness_x_explainability/encoder_results_{data}\"\n",
    "debiased_dir = f\"/scratch/yifwang/fairness_x_explainability/bias_mitigation_results_{data}\"\n",
    "\n",
    "\n",
    "models = [\"bert\", \"roberta\"] # [\"bert\", \"roberta\", \"distilbert\"]\n",
    "bias_types = [\"race\", \"gender\", \"religion\"]\n",
    "\n",
    "split = \"test\"  # \"val\" or \"test\"\n",
    "debiasing_methods = [\"no_debiasing\", \"group_balance\", \"group_class_balance\", \"cda\", \"dropout\", \"attention_entropy\", \"causal_debias\"]\n",
    "\n",
    "explanation_methods = [\"Saliency\", \"InputXGradient\", \"raw_attention\", \"attention_rollout\", \"attention_flow\", \"Occlusion\"]\n",
    "aggregation = [\"L1\", \"L2\"]\n",
    "alphas = [100.0, 10.0, 1.0, 0.1, 0.01]\n",
    "seeds = [42, 1, 2]\n",
    "\n",
    "training_types = [\"one axis\"] # [\"all axes\", \"one axis\"]\n",
    "if data == \"civil\":\n",
    "    num_examples_test = {\"race\": 2000, \"gender\": 2000, \"religion\": 1000}\n",
    "elif data == \"jigsaw\":\n",
    "    num_examples_test = {\"race\": 400, \"gender\": 800, \"religion\": 200}\n",
    "\n",
    "if data == \"civil\":\n",
    "    num_examples_val = {\"race\": 500, \"gender\": 500, \"religion\": 200}\n",
    "elif data == \"jigsaw\":\n",
    "    num_examples_val = {\"race\": 200, \"gender\": 200, \"religion\": 200}\n",
    "\n",
    "num_examples = num_examples_test if split==\"test\" else num_examples_val\n",
    "\n",
    "fairness_metrics = [\"accuracy\", \"fpr\", \"fnr\", \"individual_fairness\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87cc2a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_relative_fairness(mitigated_fairness_dict, orig_fairness_dict):\n",
    "    # 越大越好\n",
    "    relative_fairness = {}\n",
    "    for metric in mitigated_fairness_dict:\n",
    "        if metric not in orig_fairness_dict:\n",
    "            continue\n",
    "        orig_value = orig_fairness_dict[metric]\n",
    "        mitigated_value = mitigated_fairness_dict[metric]\n",
    "        if orig_value == 0:\n",
    "            relative_fairness[metric] = 0\n",
    "        else:\n",
    "            relative_fairness[metric] = (orig_value - mitigated_value) / abs(orig_value) * 100\n",
    "    if len(relative_fairness) == 0:\n",
    "        return 0\n",
    "    average_relative_fairness = sum(relative_fairness.values()) / len(relative_fairness)\n",
    "    return average_relative_fairness\n",
    "\n",
    "def compute_average_fairness(mitigated_fairness_dict, orig_fairness_dict):\n",
    "    # 越大越好\n",
    "    relative_fairness = {}\n",
    "    for metric in mitigated_fairness_dict:\n",
    "        if metric not in orig_fairness_dict:\n",
    "            continue\n",
    "        orig_value = orig_fairness_dict[metric]\n",
    "        mitigated_value = mitigated_fairness_dict[metric]\n",
    "        if orig_value == 0:\n",
    "            relative_fairness[metric] = 0\n",
    "        else:\n",
    "            relative_fairness[metric] = (orig_value - mitigated_value) * 100\n",
    "    if len(relative_fairness) == 0:\n",
    "        return 0\n",
    "    average_relative_fairness = sum(relative_fairness.values()) / len(relative_fairness)\n",
    "    return average_relative_fairness\n",
    "\n",
    "def compute_harmonic_mean_performance_fairness(performance_metric_dict, fairness_metric_dict):\n",
    "    # 越大越好\n",
    "    if len(performance_metric_dict) == 0 or len(fairness_metric_dict) == 0:\n",
    "        return 0\n",
    "    # compute harmonic mean of performance and fairness separately\n",
    "    harmonic_mean_performance = len(performance_metric_dict) / sum(1.0 / (v * 100) for v in performance_metric_dict.values() if v != 0)\n",
    "    harmonic_mean_fairness = len(fairness_metric_dict) / sum(1.0 / (100-v*100) for v in fairness_metric_dict.values() if v != 0)\n",
    "    if harmonic_mean_performance == 0 or harmonic_mean_fairness == 0:\n",
    "        return 0\n",
    "    harmonic_mean = 2 * (harmonic_mean_performance * harmonic_mean_fairness) / (harmonic_mean_performance + harmonic_mean_fairness)\n",
    "    return harmonic_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f42230b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: /scratch/yifwang/fairness_x_explainability/bias_mitigation_results_civil/roberta_civil_gender_gender_test_2000_2/Saliency/L1_1.0/fairness/fairness_gender_test_summary_stats.json\n"
     ]
    }
   ],
   "source": [
    "fairness_dict = {\"model\": [], \"bias_type\": [], \"debiasing_method\": [], \"training_data\": [], \"aggregation\": [], \"alpha\": [], \"metrics\": [], \"score\": [], \"seed\": []}\n",
    "for model in models:\n",
    "    for bias_type in bias_types:\n",
    "        groups = SOCIAL_GROUPS[bias_type]\n",
    "        # load baseline fairness results\n",
    "        for training_type in training_types:\n",
    "            data_token = \"all\" if training_type == \"all axes\" else bias_type\n",
    "            for debiasing_method in debiasing_methods:\n",
    "                \n",
    "                file_path = os.path.join(root_dir, f\"{model}_{data}_{data_token}_{bias_type}_{split}_{num_examples[bias_type]}\", debiasing_method, \"fairness\", f\"fairness_{bias_type}_{split}_summary_stats.json\")\n",
    "                if not os.path.exists(file_path):\n",
    "                    print(f\"File not found: {file_path}\")\n",
    "                    continue\n",
    "                with open(file_path, \"r\") as f:\n",
    "                    fairness_data = json.load(f)\n",
    "                \n",
    "                \n",
    "                for metric in [\"accuracy\", \"f1\"]:\n",
    "                    fairness_dict['model'].append(model)\n",
    "                    fairness_dict['bias_type'].append(bias_type)\n",
    "                    fairness_dict['debiasing_method'].append(debiasing_method)\n",
    "                    fairness_dict['training_data'].append(training_type)\n",
    "                    fairness_dict['metrics'].append(f\"task_{metric}\")\n",
    "                    fairness_dict['seed'].append(42)\n",
    "                    fairness_dict['aggregation'].append(\"N/A\")\n",
    "                    fairness_dict['alpha'].append(\"N/A\")\n",
    "                    if metric in fairness_data['Metrics']['overall']:\n",
    "                        fairness_dict['score'].append(fairness_data['Metrics']['overall'][metric])\n",
    "                    else:\n",
    "                        print(f\"Metric {metric} not found in fairness data for {debiasing_method} on {bias_type}\")\n",
    "                        fairness_dict['score'].append(None)\n",
    "\n",
    "                for metric in fairness_metrics:\n",
    "                    fairness_dict['model'].append(model)\n",
    "                    fairness_dict['bias_type'].append(bias_type)\n",
    "                    fairness_dict['debiasing_method'].append(debiasing_method)\n",
    "                    fairness_dict['training_data'].append(training_type)\n",
    "                    fairness_dict[\"seed\"].append(42)\n",
    "                    fairness_dict['aggregation'].append(\"N/A\")\n",
    "                    fairness_dict['alpha'].append(\"N/A\")\n",
    "                    if metric != \"individual_fairness\":\n",
    "                        fairness_dict['metrics'].append(metric)\n",
    "                        fairness_dict['score'].append(sum([abs(fairness_data['Group_Fairness'][\"average\"][group][metric]) for group in groups]))\n",
    "                    else:\n",
    "                        fairness_dict['metrics'].append(\"individual_fairness\")\n",
    "                        fairness_dict['score'].append(fairness_data['Individual_Fairness']['overall'][\"predicted_class\"][\"abs_average\"])\n",
    "\n",
    "            for explanation_method in explanation_methods:\n",
    "                for aggregation_method in aggregation:\n",
    "                    if aggregation_method == \"L2\" and explanation_method in [\"raw_attention\", \"attention_rollout\", \"attention_flow\", \"Occlusion\"]:\n",
    "                            continue\n",
    "                    for alpha in alphas:\n",
    "                        for seed in seeds:\n",
    "                            if aggregation_method == \"L1\" and explanation_method in [\"raw_attention\", \"attention_rollout\", \"attention_flow\", \"Occlusion\"]:\n",
    "                                file_path = os.path.join(debiased_dir, f\"{model}_{data}_{data_token}_{bias_type}_{split}_{num_examples[bias_type]}_{seed}\", explanation_method, f\"{alpha}\",\"fairness\", f\"fairness_{bias_type}_{split}_summary_stats.json\")\n",
    "                            else:\n",
    "                                file_path = os.path.join(debiased_dir, f\"{model}_{data}_{data_token}_{bias_type}_{split}_{num_examples[bias_type]}_{seed}\", explanation_method, f\"{aggregation_method}_{alpha}\",\"fairness\", f\"fairness_{bias_type}_{split}_summary_stats.json\")\n",
    "                            if not os.path.exists(file_path):\n",
    "                                print(f\"File not found: {file_path}\")\n",
    "                                continue\n",
    "                            with open(file_path, \"r\") as f:\n",
    "                                explanation_data = json.load(f)\n",
    "                            \n",
    "                            for metric in [\"accuracy\", \"f1\"]:\n",
    "                                fairness_dict['model'].append(model)\n",
    "                                fairness_dict['bias_type'].append(bias_type)\n",
    "                                fairness_dict['debiasing_method'].append(f\"{explanation_method}\")\n",
    "                                fairness_dict['training_data'].append(training_type)\n",
    "                                fairness_dict['metrics'].append(f\"task_{metric}\")\n",
    "                                fairness_dict['seed'].append(seed)\n",
    "                                fairness_dict['aggregation'].append(f\"{aggregation_method}\")\n",
    "                                fairness_dict['alpha'].append(f\"{alpha}\")\n",
    "                                if metric in explanation_data['Metrics']['overall']:\n",
    "                                    fairness_dict['score'].append(explanation_data['Metrics']['overall'][metric])\n",
    "                                else:\n",
    "                                    print(f\"Metric {metric} not found in explanation data for {explanation_method} on {bias_type}\")\n",
    "                                    fairness_dict['score'].append(None)\n",
    "\n",
    "                            for metric in fairness_metrics:\n",
    "                                fairness_dict['model'].append(model)\n",
    "                                fairness_dict['bias_type'].append(bias_type)\n",
    "                                fairness_dict['debiasing_method'].append(f\"{explanation_method}\")\n",
    "                                fairness_dict['training_data'].append(training_type)\n",
    "                                fairness_dict['seed'].append(seed)\n",
    "                                fairness_dict['aggregation'].append(f\"{aggregation_method}\")\n",
    "                                fairness_dict['alpha'].append(f\"{alpha}\")\n",
    "                                if metric != \"individual_fairness\":\n",
    "                                    fairness_dict['metrics'].append(metric)\n",
    "                                    fairness_dict['score'].append(sum([abs(explanation_data['Group_Fairness'][\"average\"][group][metric]) for group in groups]))\n",
    "                                    \n",
    "                                else:\n",
    "                                    fairness_dict['metrics'].append(\"individual_fairness\")\n",
    "                                    fairness_dict['score'].append(explanation_data['Individual_Fairness']['overall'][\"predicted_class\"][\"abs_average\"])\n",
    "\n",
    "\n",
    "# convert to a pandas DataFrame\n",
    "import pandas as pd\n",
    "fairness_df = pd.DataFrame(fairness_dict)\n",
    "# fairness_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a190123d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "================================\n",
      "Selected fairness metrics: ['accuracy']\n",
      "================================\n",
      "Best average fairness: -0.04999999999999449 with method Saliency_L2_1.0\n",
      "Best harmonic mean of performance and fairness: 86.7933466022178 with method Saliency_L2_1.0\n",
      "================================\n",
      "Original model results:\n",
      "{'task_accuracy': np.float64(0.78375)}\n",
      "{'accuracy': np.float64(0.020500000000000074)}\n",
      "================================\n",
      "Best average fairness results:\n",
      "Saliency_L2_1.0\n",
      "{'task_accuracy': np.float64(0.7795)}\n",
      "{'accuracy': np.float64(0.02100000000000002)}\n",
      "================================\n",
      "Best harmonic mean results:\n",
      "Saliency_L2_1.0\n",
      "{'task_accuracy': np.float64(0.7795)}\n",
      "{'accuracy': np.float64(0.02100000000000002)}\n",
      "original harmonic mean:  87.07592513823904\n",
      "bias mitigated harmonic mean:  86.7933466022178\n",
      "\n",
      "\n",
      "\n",
      "================================\n",
      "Selected fairness metrics: ['fpr']\n",
      "================================\n",
      "Best average fairness: 0.16380979253372147 with method Saliency_L2_10.0\n",
      "Best harmonic mean of performance and fairness: 87.4492122379971 with method Saliency_L2_0.1\n",
      "================================\n",
      "Original model results:\n",
      "{'task_accuracy': np.float64(0.78375)}\n",
      "{'fpr': np.float64(0.005016185034525458)}\n",
      "================================\n",
      "Best average fairness results:\n",
      "Saliency_L2_10.0\n",
      "{'task_accuracy': np.float64(0.77275)}\n",
      "{'fpr': np.float64(0.0033780871091882436)}\n",
      "================================\n",
      "Best harmonic mean results:\n",
      "Saliency_L2_0.1\n",
      "{'task_accuracy': np.float64(0.78275)}\n",
      "{'fpr': np.float64(0.009405346939735243)}\n",
      "original harmonic mean:  87.68243549632265\n",
      "bias mitigated harmonic mean:  87.4492122379971\n",
      "\n",
      "\n",
      "\n",
      "================================\n",
      "Selected fairness metrics: ['fnr']\n",
      "================================\n",
      "Best average fairness: 3.0039168007957917 with method Saliency_L2_1.0\n",
      "Best harmonic mean of performance and fairness: 84.7956436007805 with method Saliency_L2_1.0\n",
      "================================\n",
      "Original model results:\n",
      "{'task_accuracy': np.float64(0.78375)}\n",
      "{'fnr': np.float64(0.10044487120149764)}\n",
      "================================\n",
      "Best average fairness results:\n",
      "Saliency_L2_1.0\n",
      "{'task_accuracy': np.float64(0.7795)}\n",
      "{'fnr': np.float64(0.07040570319353973)}\n",
      "================================\n",
      "Best harmonic mean results:\n",
      "Saliency_L2_1.0\n",
      "{'task_accuracy': np.float64(0.7795)}\n",
      "{'fnr': np.float64(0.07040570319353973)}\n",
      "original harmonic mean:  83.76690834406891\n",
      "bias mitigated harmonic mean:  84.7956436007805\n",
      "\n",
      "\n",
      "\n",
      "================================\n",
      "Selected fairness metrics: ['individual_fairness']\n",
      "================================\n",
      "Best average fairness: 0.6798416376113892 with method Saliency_L2_100.0\n",
      "Best harmonic mean of performance and fairness: 86.36495999235147 with method Saliency_L2_0.1\n",
      "================================\n",
      "Original model results:\n",
      "{'task_accuracy': np.float64(0.78375)}\n",
      "{'individual_fairness': np.float64(0.03168902546167374)}\n",
      "================================\n",
      "Best average fairness results:\n",
      "Saliency_L2_100.0\n",
      "{'task_accuracy': np.float64(0.768)}\n",
      "{'individual_fairness': np.float64(0.024890609085559845)}\n",
      "================================\n",
      "Best harmonic mean results:\n",
      "Saliency_L2_0.1\n",
      "{'task_accuracy': np.float64(0.78275)}\n",
      "{'individual_fairness': np.float64(0.03680082783102989)}\n",
      "original harmonic mean:  86.63097201789903\n",
      "bias mitigated harmonic mean:  86.36495999235147\n"
     ]
    }
   ],
   "source": [
    "selected_model = \"bert\"\n",
    "selected_bias_type = \"race\"\n",
    "selected_explanation_method = \"Saliency\"\n",
    "selected_training_data = \"one axis\"\n",
    "selected_aggregation = \"L2\"\n",
    "if selected_aggregation == \"L2\" and selected_explanation_method in [\"raw_attention\", \"attention_rollout\", \"attention_flow\", \"Occlusion\"]:\n",
    "    selected_aggregation = \"L1\"\n",
    "\n",
    "selected_seed = 42\n",
    "\n",
    "selected_performance_metrics = [\"task_accuracy\"]\n",
    "\n",
    "for selected_fairness_metrics in [[\"accuracy\"], [\"fpr\"], [\"fnr\"], [\"individual_fairness\"]]:\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"================================\")\n",
    "    print(f\"Selected fairness metrics: {selected_fairness_metrics}\")\n",
    "    print(\"================================\")\n",
    "\n",
    "    # original fairness results\n",
    "    orig_fairness_result = fairness_df[(fairness_df['model'] == selected_model) & (fairness_df['bias_type'] == selected_bias_type) & (fairness_df['debiasing_method'] == 'no_debiasing') & (fairness_df['training_data'] == selected_training_data)]\n",
    "    orig_fairness_metric = {metric: orig_fairness_result[orig_fairness_result['metrics'] == metric]['score'].values[0] for metric in selected_fairness_metrics}\n",
    "    orig_performance_metric = {metric: orig_fairness_result[orig_fairness_result['metrics'] == metric]['score'].values[0] for metric in selected_performance_metrics}\n",
    "    orig_harmonic_mean = compute_harmonic_mean_performance_fairness(orig_performance_metric, orig_fairness_metric)\n",
    "    best_avg_fairness = -float('inf')\n",
    "    best_avg_relative_fairness = -float('inf')\n",
    "    best_harmonic_mean = -float('inf')\n",
    "    best_avg_fairness_debiasing_method = None\n",
    "    best_avg_relative_fairness_debiasing_method = None\n",
    "    best_harmonic_mean_debiasing_method = None\n",
    "    best_avg_fairness_fairness_metric = {}\n",
    "    best_avg_fairness_performance_metric = {}\n",
    "    best_avg_relative_fairness_fairness_metric = {}\n",
    "    best_avg_relative_fairness_performance_metric = {}\n",
    "    best_harmonic_mean_fairness_metric = {}\n",
    "    best_harmonic_mean_performance_metric = {}\n",
    "    for alpha in alphas:\n",
    "        mitigated_fairness_result = fairness_df[(fairness_df['model'] == selected_model) & (fairness_df['bias_type'] == selected_bias_type) & (fairness_df['debiasing_method'] == selected_explanation_method) & (fairness_df['training_data'] == selected_training_data) & (fairness_df['aggregation'] == selected_aggregation) & (fairness_df['seed'] == selected_seed) & (fairness_df['alpha'] == str(alpha))]\n",
    "        mitigated_fairness_metric = {metric: mitigated_fairness_result[mitigated_fairness_result['metrics'] == metric]['score'].values[0] for metric in selected_fairness_metrics}\n",
    "        \n",
    "        mitigated_performance_metric = {metric: mitigated_fairness_result[mitigated_fairness_result['metrics'] == metric]['score'].values[0] for metric in selected_performance_metrics}\n",
    "        avg_fairness = compute_average_fairness(mitigated_fairness_metric, orig_fairness_metric)\n",
    "        #avg_relative_fairness = compute_average_relative_fairness(mitigated_fairness_metric, orig_fairness_metric)\n",
    "        harmonic_mean = compute_harmonic_mean_performance_fairness(mitigated_performance_metric, mitigated_fairness_metric)\n",
    "        if avg_fairness > best_avg_fairness:\n",
    "            best_avg_fairness = avg_fairness\n",
    "            best_avg_fairness_debiasing_method = f\"{selected_explanation_method}_{selected_aggregation}_{alpha}\"\n",
    "            best_avg_fairness_fairness_metric = mitigated_fairness_metric\n",
    "            best_avg_fairness_performance_metric = mitigated_performance_metric\n",
    "        # if avg_relative_fairness > best_avg_relative_fairness:\n",
    "        #     best_avg_relative_fairness = avg_relative_fairness\n",
    "        #     best_avg_relative_fairness_debiasing_method = f\"{selected_explanation_method}_{selected_aggregation}_{alpha}\"\n",
    "        #     best_avg_relative_fairness_fairness_metric = mitigated_fairness_metric\n",
    "        #     best_avg_relative_fairness_performance_metric = mitigated_performance_metric\n",
    "        if harmonic_mean > best_harmonic_mean:\n",
    "            best_harmonic_mean = harmonic_mean\n",
    "            best_harmonic_mean_debiasing_method = f\"{selected_explanation_method}_{selected_aggregation}_{alpha}\"\n",
    "            best_harmonic_mean_fairness_metric = mitigated_fairness_metric\n",
    "            best_harmonic_mean_performance_metric = mitigated_performance_metric\n",
    "    print(f\"Best average fairness: {best_avg_fairness} with method {best_avg_fairness_debiasing_method}\")\n",
    "    #print(f\"Best average relative fairness: {best_avg_relative_fairness} with method {best_avg_relative_fairness_debiasing_method}\")\n",
    "    print(f\"Best harmonic mean of performance and fairness: {best_harmonic_mean} with method {best_harmonic_mean_debiasing_method}\")\n",
    "    print(\"================================\")\n",
    "    # show original model results\n",
    "    print(\"Original model results:\")\n",
    "    print(orig_performance_metric)\n",
    "    print(orig_fairness_metric)\n",
    "\n",
    "    print(\"================================\")\n",
    "    # show best average fairness results\n",
    "    print(\"Best average fairness results:\")\n",
    "    print(best_avg_fairness_debiasing_method)\n",
    "    print(best_avg_fairness_performance_metric)\n",
    "    print(best_avg_fairness_fairness_metric)\n",
    "\n",
    "    # print(\"================================\")\n",
    "    # # show best average relative fairness results\n",
    "    # print(\"Best average relative fairness results:\")\n",
    "    # print(best_avg_relative_fairness_debiasing_method)\n",
    "    # print(best_avg_relative_fairness_performance_metric)\n",
    "    # print(best_avg_relative_fairness_fairness_metric)\n",
    "\n",
    "    print(\"================================\")\n",
    "    # show best harmonic mean results\n",
    "    print(\"Best harmonic mean results:\")\n",
    "    print(best_harmonic_mean_debiasing_method)\n",
    "    print(best_harmonic_mean_performance_metric)\n",
    "    print(best_harmonic_mean_fairness_metric)\n",
    "    print(\"original harmonic mean: \", orig_harmonic_mean)\n",
    "    print(\"bias mitigated harmonic mean: \", best_harmonic_mean)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bcos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
